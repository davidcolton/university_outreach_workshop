{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IBM University Outreach Workshop","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to our workshop! In this workshop we'll be using the open-sourced IBM Granite AI foundation models and Mellea for a number of use cases that demonstrates the value of generative AI.</p> <p>By the end of this workshop, you will be able to:</p> <ul> <li>Run a simple \"Hello Mellea\" program</li> <li>Use Mellea to Instruct, Validate, Repair</li> <li>Have some fun with <code>@generative</code> functions</li> <li>Use Mellea and Docling</li> <li>...</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Introduction</li> <li>About this workshop</li> <li>Agenda</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Link to Content Description Lab 0: Pre-work Pre-work for the workshop"},{"location":"#technology-used","title":"Technology Used","text":"<p>The technology used in the workshop is as follows:</p> <ul> <li>IBM Granite AI foundation models](https://www.ibm.com/granite)</li> <li>Mellea</li> <li>Docling</li> <li>Jupyter notebooks</li> <li>LangChain</li> <li>Ollama</li> </ul>"},{"location":"lab-3/readme/","title":"Energy Demand Forecasting with Granite Timeseries (TTM)","text":"<p>TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. With less than 1 Million parameters, TTM introduces the notion of the first-ever \"tiny\" pre-trained models for Time-Series Forecasting. TTM outperforms several popular benchmarks demanding billions of parameters in zero-shot and few-shot forecasting and can easily be fine-tuned for multi-variate forecasts.</p>"},{"location":"lab-3/readme/#install-the-tsfm-library","title":"Install the TSFM Library","text":"<p>The granite-tsfm library provides utilities for working with Time Series Foundation Models (TSFM). Here the pinned version is retrieved and installed.</p>"},{"location":"lab-3/readme/#install-the-tsfm-library_1","title":"Install the tsfm library","text":"<pre><code>import sys\nif 'google.colab' in sys.modules:\n    ! pip install --force-reinstall --no-warn-conflicts \"numpy&lt;2.1.0,&gt;=2.0.2\"\n</code></pre> <ol> <li>This code snippet uses the \"granite-tsfm[notebooks]==v0.2.22\" library.</li> <li>The code imports the sys module and </li> <li>Checks if the google.colab module is present in the sys.modules dictionary and if found, it installs the numpy library with a specific version range (&lt;2.1.0, &gt;=2.0.2) using the pip  package manager. The --force-reinstall flag forces a reinstallation of the package, and the --no-warn-conflicts flag suppresses warnings about conflicting dependencies.</li> </ol>"},{"location":"lab-3/readme/#import-packages","title":"Import Packages","text":"<p>From <code>tsfm_public</code>, we use the TinyTimeMixer model, forecasting pipeline, and plotting function.</p> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\nfrom tsfm_public import (\n    TimeSeriesForecastingPipeline,\n    TinyTimeMixerForPrediction,\n)\nfrom tsfm_public.toolkit.visualization import plot_predictions\n</code></pre> <ol> <li>The first three lines in the snippet of code imports the matplotlib.pyplot module as plt, the pandas library as pd and the torch library. These libraries are utilizied for the data visualizations, dataframes and working with tensors which provides-multidimensional arrays respectively.</li> <li>The fourth line imports the TimeSeriesForecastingPipeline and TinyTimeMixerForPrediction classes from the tsfm_public module. These classes are part of the tsfm_public library, which is a collection of time series forecasting models and utilities.</li> <li>The fifth line imports the plot_predictions function from the visualization submodule of the tsfm_public library. </li> </ol> <p>This function is used to visualize the forecasted time series data.This code snippet imports the necessary libraries and classes for working with time series data, performing forecasting, and visualizing the results. The imported libraries and classes include data manipulation (Pandas), data visualization (Matplotlib), deep learning (Torch), and time series forecasting (tsfm_public).</p>"},{"location":"lab-3/readme/#download-the-data","title":"Download the data","text":"<p>We'll work with a dataset of hourly electrical demand, generation by type, and weather in Spain. This dataset was originally available from Kaggle. To simplify access to the data, we will make use of the (energy consumption and weather) datasets on Hugging Face.</p> <p><pre><code>DATA_FILE_PATH = \"hf://datasets/vitaliy-sharandin/energy-consumption-hourly-spain/energy_dataset.csv\"\n</code></pre> The code snippet defines a variable named DATA_FILE_PATH and assigns it a string value. This is a Hugging Face dataset identifier, which follows the format hf://datasets/username/dataset_name/dataset_file.format</p> <p><code>hf://datasets/vitaliy-sharandin/energy-consumption-hourly-spain/energy_dataset.csv</code>: points to a specific CSV file containing energy consumption data for Spain, with hourly granularity. </p>"},{"location":"lab-3/readme/#specify-time-and-output-variables","title":"Specify time and output variables","text":"<p>We provide the names of the timestamp column and the target column to be predicted. The context length (in time steps) is set to match the pretrained model.</p> <pre><code>timestamp_column = \"time\"\ntarget_columns = [\"total load actual\"]\ncontext_length = 512\n</code></pre> <ol> <li><code>timestamp_column</code>: This variable is assigned the string value \"time\". It represents the name of the column in the dataset that contains the timestamp information for each data point. In this case, the timestamp column is named \"time\".</li> <li><code>target_columns</code>: This variable is assigned a list containing the string value \"total load actual\". It represents the name of the column(s) in the dataset that contain the target variable(s) for the forecasting task. In this case, the target column is named \"total load actual\", which likely represents the actual total energy consumption at each time point.</li> <li><code>context_length</code>: This variable is assigned the integer value 512. It represents the number of time steps that the model will consider as context when generating forecasts. In this case, the context length is set to 512, meaning that the model will use the previous 512 time steps to predict the next time step.</li> </ol> <p>In summary, the code snippet defines three variables: timestamp_column, target_columns, and context_length. These variables are used to specify the timestamp column, target column(s), and context length for a time series forecasting task using the TinyTimeMixer model.</p>"},{"location":"lab-3/readme/#read-in-the-data","title":"Read in the data","text":"<p>We parse the csv into a pandas dataframe, filling in any null values, and create a single window containing <code>context_length</code> time points. We ensure the timestamp column is a datetime.</p> <pre><code># Read in the data from the downloaded file.\ninput_df = pd.read_csv(\n    DATA_FILE_PATH,\n    parse_dates=[timestamp_column],  # Parse the timestamp values as dates.\n)\n\n# Fill NA/NaN values by propagating the last valid value.\ninput_df = input_df.ffill()\n\n# Only use the last `context_length` rows for prediction.\ninput_df = input_df.iloc[-context_length:,]\n\n# Show the last few rows of the dataset.\ninput_df.tail()\n</code></pre> <ol> <li><code>pd.read_csv()</code>: This function from the Pandas library is used to read a CSV file into a DataFrame. In this case, it reads the data from the file specified by <code>DATA_FILE_PATH</code>.</li> <li><code>parse_dates=[timestamp_column]</code>: This argument is passed to the <code>pd.read_csv()</code> function to specify that the column named <code>timestamp_column</code> should be parsed as dates. This ensures that the timestamp values are treated as datetime objects, allowing for more accurate time-based operations and visualizations.</li> <li><code>input_df.ffill()</code>: This method is called on the <code>input_df</code> DataFrame to fill any missing or NaN values by propagating the last valid value. This is a common technique for handling missing data in time series datasets, as it maintains the temporal order of the data.</li> <li><code>input_df.iloc[-context_length:, ]</code>: This method is used to select the last <code>context_length</code> rows from the <code>input_df</code> DataFrame. This ensures that only the most recent data is used for prediction, as the TinyTimeMixer model requires a fixed-length context window.</li> <li><code>input_df.tail()</code>: This method is called on the <code>input_df</code> DataFrame to display the last few rows of the dataset. This can be useful for quickly verifying that the data has been correctly loaded, preprocessed, and filtered.</li> </ol> <p>In summary, the code snippet reads a CSV file containing time series data, parses the timestamp column as dates, fills missing values using forward propagation, selects the last <code>context_length</code> rows for prediction, and displays the last few rows of the dataset. These steps prepare the data for use with the TinyTimeMixer model in a time series forecasting task.</p>"},{"location":"lab-3/readme/#plot-the-target-series","title":"Plot the target series","text":"<p>Here we inspect a preview of the target time series column.</p> <pre><code>fig, axs = plt.subplots(len(target_columns), 1, figsize=(10, 2 * len(target_columns)), squeeze=False)\nfor ax, target_column in zip(axs, target_columns):\n    ax[0].plot(input_df[timestamp_column], input_df[target_column])\n</code></pre> <ol> <li><code>plt.subplots()</code>: This function from the Matplotlib library is used to create a figure and a set of subplots. In this case, it creates a figure with a grid of subplots, where the number of rows is equal to the length of the <code>target_columns</code> list, and the number of columns is 1. The <code>figsize</code> argument is used to set the size of the figure, and the <code>squeeze</code> argument is set to <code>False</code> to ensure that the returned axes object is always a 2D array, even if there is only one subplot.</li> <li><code>for ax, target_column in zip(axs, target_columns)</code>: This loop iterates over the rows of the <code>axs</code> array (i.e., the subplots) and the <code>target_columns</code> list simultaneously using the <code>zip()</code> function. For each iteration, the loop assigns the current subplot (<code>ax</code>) and the corresponding target column (<code>target_column</code>) to the variables <code>ax</code> and <code>target_column</code>, respectively.</li> <li><code>ax[0].plot(input_df[timestamp_column], input_df[target_column])</code>: Inside the loop, this line plots the time series data for the current target column on the corresponding subplot. The <code>input_df[timestamp_column]</code> expression retrieves the timestamp values, and the <code>input_df[target_column]</code> expression retrieves the target column values. The <code>plot()</code> method is called on the current subplot (<code>ax[0]</code>) to create the line plot.</li> </ol> <p>In summary, the code snippet creates a figure with a grid of subplots, one for each target column, and plots the time series data for each target column on its corresponding subplot. This visualization helps to compare the trends and patterns in the target columns over time.</p>"},{"location":"lab-3/readme/#set-up-zero-shot-model","title":"Set up zero shot model","text":"<p>The TTM model is hosted on Hugging Face, and is retrieved by the wrapper, <code>TinyTimeMixerForPrediction</code>. We have one input channel in this example.</p> <p><pre><code># Instantiate the model.\nzeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\n    \"ibm-granite/granite-timeseries-ttm-r2\",  # Name of the model on Hugging Face\n    num_input_channels=len(target_columns),  # tsp.num_input_channels\n)\n</code></pre> 1. <code>TinyTimeMixerForPrediction.from_pretrained()</code>: This method is used to instantiate a pre-trained TinyTimeMixer model from the Hugging Face Model Hub. The method takes the following arguments:     * <code>\"ibm-granite/granite-timeseries-ttm-r2\"</code>: The name of the pre-trained model on the Hugging Face Model Hub. In this case, it is the TinyTimeMixer model pre-trained on time series data by IBM Granite.     * <code>num_input_channels</code>: The number of input channels in the time series data. This value is set to the length of the <code>target_columns</code> list, indicating that the model will process multiple target columns as separate input channels. 2. <code>zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(...)</code>: This line instantiates the pre-trained TinyTimeMixer model and assigns it to the <code>zeroshot_model</code> variable. The model is now ready to be used for forecasting tasks.</p> <p>In summary, the code snippet instantiates a pre-trained TinyTimeMixer model from the Hugging Face Model Hub, specifying the number of input channels based on the number of target columns in the dataset. The instantiated model is stored in the <code>zeroshot_model</code> variable and can be used for generating forecasts.</p>"},{"location":"lab-3/readme/#create-a-forecasting-pipeline","title":"Create a forecasting pipeline","text":"<p>Set up the forecasting pipeline with the model, setting <code>frequency</code> given our knowledge of the sample frequency. In this example we set <code>explode_forecasts</code> to <code>False</code>, which keeps each sequence of predictions in a list within the dataframe cells. We then make a forecast on the dataset.</p> <pre><code># Create a pipeline.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npipeline = TimeSeriesForecastingPipeline(\n    zeroshot_model,\n    timestamp_column=timestamp_column,\n    id_columns=[],\n    target_columns=target_columns,\n    explode_forecasts=False,\n    freq=\"h\",\n    device=device,  # Specify your local GPU or CPU.\n)\n\n# Make a forecast on the target column given the input data.\nzeroshot_forecast = pipeline(input_df)\nzeroshot_forecast.tail()\n</code></pre> <ol> <li><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"</code>: This line checks if a GPU is available for use and sets the <code>device</code> variable accordingly. If a GPU is available, the <code>device</code> variable is set to <code>\"cuda\"</code>; otherwise, it is set to <code>\"cpu\"</code>. This ensures that the forecasting pipeline uses the available hardware for faster computation.</li> <li><code>TimeSeriesForecastingPipeline</code>: This is a class from the <code>tsfm_public</code> library that represents a high-level interface for building and executing time series forecasting pipelines. The pipeline consists of several components, such as data preprocessing, feature engineering, model training, and forecast generation.</li> <li><code>pipeline = TimeSeriesForecastingPipeline(...)</code>: This line creates an instance of the <code>TimeSeriesForecastingPipeline</code> class, passing the required arguments to configure the pipeline. The arguments include:<ul> <li><code>zeroshot_model</code>: The TinyTimeMixer model used for forecasting.</li> <li><code>timestamp_column</code>: The name of the column containing the timestamp information.</li> <li><code>id_columns</code>: A list of columns that do not contain time series data and should not be used for forecasting. In this case, it is an empty list.</li> <li><code>target_columns</code>: A list of columns containing the target time series data for forecasting.</li> <li><code>explode_forecasts</code>: A boolean flag indicating whether to generate separate forecasts for each unique identifier in the <code>id_columns</code>. In this case, it is set to <code>False</code>.</li> <li><code>freq</code>: The frequency of the time series data, specified as a string (e.g., <code>\"h\"</code> for hourly data).</li> <li><code>device</code>: The device to use for computation, either <code>\"cuda\"</code> for a GPU or <code>\"cpu\"</code> for the CPU.</li> </ul> </li> <li><code>zeroshot_forecast = pipeline(input_df)</code>: This line uses the configured pipeline to generate forecasts for the target columns in the <code>input_df</code> DataFrame. The resulting forecasts are stored in the <code>zeroshot_forecast</code> variable.</li> <li><code>zeroshot_forecast.tail()</code>: This method is called on the <code>zeroshot_forecast</code> object to display the last few rows of the forecasted data. This can be useful for quickly verifying that the forecasts have been generated correctly.</li> </ol> <p>In summary, the code snippet creates a time series forecasting pipeline using the TinyTimeMixer model, configures the pipeline with the required parameters, generates forecasts for the target columns in the input data, and displays the last few rows of the forecasted data.</p>"},{"location":"lab-3/readme/#plot-predictions-along-with-the-historical-data","title":"Plot predictions along with the historical data.","text":"<p>The predicted series picks up where the historical data ends, and we can see that it predicts a continuation of the cyclical pattern and an upward trend.</p> <pre><code># Plot the historical data and predicted series.\nplot_predictions(\n    input_df=input_df,\n    predictions_df=zeroshot_forecast,\n    freq=\"h\",\n    timestamp_column=timestamp_column,\n    channel=target_column,\n    indices=[-1],\n    num_plots=1,\n)\n</code></pre> <ol> <li><code>plot_predictions()</code>: This function from the <code>tsfm_public.toolkit.visualization</code> module is used to visualize the historical data and predicted series. It takes several arguments to customize the plot:<ul> <li><code>input_df</code>: The input DataFrame containing the historical time series data.</li> <li><code>predictions_df</code>: The DataFrame containing the forecasted series.</li> <li><code>freq</code>: The frequency of the time series data, specified as a string (e.g., <code>\"h\"</code> for hourly data).</li> <li><code>timestamp_column</code>: The name of the column containing the timestamp information.</li> <li><code>channel</code>: The name of the target column for which the forecasts were generated.</li> <li><code>indices</code>: A list of indices specifying which forecasts to plot. In this case, it is set to <code>[-1]</code>, which means that only the most recent forecast is plotted.</li> <li><code>num_plots</code>: The number of subplots to create. In this case, it is set to <code>1</code>, indicating that a single plot is generated.</li> </ul> </li> <li><code>plot_predictions(...)</code>: This line calls the <code>plot_predictions()</code> function, passing the required arguments to customize the plot. The function generates a plot that displays the historical data and the most recent forecast for the specified target column.</li> </ol> <p>In summary, the code snippet uses the <code>plot_predictions()</code> function to visualize the historical data and the most recent forecast for the target column in a single plot. This visualization helps to compare the actual data with the predicted series and assess the model's performance.</p>"},{"location":"lab_01/readme/","title":"What is Generative Computing?","text":"<p>A generative program is any computer program that contains calls to an LLM. As we will see throughout the documentation, LLMs can be incorporated into software in a wide variety of ways. Some ways of incorporating LLMs into programs tend to result in robust and performant systems, while others result in software that is brittle and error-prone.</p> <p>Generative programs are distinguished from classical programs by their use of functions that invoke generative models. These generative calls can produce many different data types \u2014 strings, booleans, structured data, code, images/video, and so on. The model(s) and software underlying generative calls can be combined and composed in certain situations and in certain ways (e.g., LoRA adapters as a special case). In addition to invoking generative calls, generative programs can invoke other functions, written in languages that do not have an LLM in their base, so that we can, for example, pass the output of a generative function into a DB retrieval system and feed the output of that into another generator. Writing generative programs is difficult because generative programs interleave deterministic and stochastic operations.</p> <p>If you would like to read more about this, please don't hesitate to take a look here.</p>"},{"location":"lab_01/readme/#mellea","title":"Mellea","text":"<p>Mellea is a library for writing generative programs. Generative programming replaces flaky agents and brittle prompts with structured, maintainable, robust, and efficient AI workflows.</p>"},{"location":"lab_01/readme/#features","title":"Features","text":"<ul> <li>A standard library of opinionated prompting patterns.</li> <li>Sampling strategies for inference-time scaling.</li> <li>Clean integration between verifiers and samplers.</li> <li>Batteries-included library of verifiers.</li> <li>Support for efficient checking of specialized requirements using activated LoRAs.</li> <li>Train your own verifiers on proprietary classifier data.</li> <li>Compatible with many inference services and model families. Control cost and quality by easily lifting and shifting workloads between:</li> <li>inference providers</li> <li>model families</li> <li>model sizes</li> <li>Easily integrate the power of LLMs into legacy code-bases (mify).</li> <li>Sketch applications by writing specifications and letting <code>mellea</code> fill in the details (generative slots).</li> <li>Get started by decomposing your large unwieldy prompts into structured and maintainable Mellea problems.</li> </ul>"},{"location":"lab_01/readme/#lab-1-hello-mellea","title":"Lab 1: Hello Mellea","text":"<p>Running <code>mellea.start_session()</code> initialize a new <code>MelleaSession</code>. The session holds three things:</p> <ol> <li>The model to use for this session. In this tutorial we will use granite3.3:8b.</li> <li>An inference engine; i.e., the code that actually calls our model. We will be using ollama, but you can also use Huggingface or any OpenAI-compatible endpoint.</li> <li>A <code>Context</code>, which tells Mellea how to remember context between requests. This is sometimes called the \"Message History\" in other frameworks. Throughout these early tutoriala, we will be using a <code>SimpleContext</code>. In <code>SimpleContext</code>, every request starts with a fresh context. There is no preserved chat history between requests. Mellea provides other types of context, but for now we will not be using those features. See the Mellea Tutorials for further details.</li> </ol>"},{"location":"lab_01/readme/#imports","title":"Imports","text":"<pre><code># Import necessary libraries\nimport mellea\n\n# Display utilities\nfrom IPython.display import display, Markdown\n\n# Format code cells with black\nimport jupyter_black\njupyter_black.load() \n</code></pre> <p>To use Mellea we import the <code>mellea</code> library. The other imports are only for displaying output and for formatting the notebook cells and can be ignored.</p> <p>Note</p> <p>If you see something about the Rust compiler, please confirm you are using python3.11, or python3.12 anything above that has a Rust dependency.</p>"},{"location":"lab_01/readme/#chat-with-mellea","title":"Chat with Mellea","text":"<p>We can have a simple chat with a <code>mellea</code> session by starting a session, <code>m</code>, and then using the <code>chat</code> function. We can then display the answer by displaying the answer objects content.</p> <pre><code># Create a Mellea model using the granite3.3:8b model and the ollama inference engine\nm = mellea.start_session()\n\n# Send a chat message to the model\n# In this example, we are asking for fun trivia about IBM and the early history of AI.\n# Since we are using a SimpleContext, there is no preserved chat history between requests.\n# Each request starts fresh.\nanswer = m.chat(\n    \"tell me some fun trivia about IBM and the early history of AI.\"\n)\n\n# Display the answer in markdown format\ndisplay(Markdown(answer.content))\n</code></pre>"},{"location":"lab_01/readme/#start-a-session-with-linearcontext","title":"Start a Session with LinearContext","text":"<p>In the first example we have used SimpleContext, a context manager that resets the chat message history on each model call. That is, the model's context is entirely determined by the current Component.</p> <p>Mellea also provides a LinearContext, which behaves like a chat history. We can use the ChatContext to interact with chat models</p> <pre><code>from mellea import start_session\nfrom mellea.stdlib.base import ChatContext\n\n# Create a session with chat context\nm = start_session(ctx=ChatContext())\n\n# Generate and store responses\nproblem = m.chat(\"Make up a math problem.\")\nsolution = m.chat(\"Solve your math problem.\")\n\n# Display the problem and solution using the content attribute\ndisplay(Markdown(f\"Problem:\\n{problem.content}\\nSolution:\\n{solution.content}\"))\n</code></pre>"},{"location":"lab_02/readme/","title":"Lab 2: Instruct-Validate-Repair","text":"<p>Instruct-Validate-Repair is a design pattern for building robust automation using LLMs. The idea is simple:</p> <ol> <li>Instruct the model to perform a task and specify requirements on the output of the task.</li> <li>Validate that these requirements are satisfied by the model's output.</li> <li>If any requirements fail, try to repair.</li> </ol>"},{"location":"lab_02/readme/#imports","title":"Imports","text":"<p>In addition to importing <code>mellea</code> from <code>stdlib.requirements</code> we import <code>check</code>, <code>req</code> and <code>simple_validate</code>. These provide syntactic sugar for writing validation functions that operate over the last output from the model (interpreted as a string).</p> <p>This is useful when your validation logic only depends upon the most recent model output.</p> <pre><code># Import necessary libraries\nimport mellea\nfrom mellea.stdlib.requirement import check, req, simple_validate\nfrom mellea.stdlib.sampling import RejectionSamplingStrategy\n</code></pre>"},{"location":"lab_02/readme/#define-requirements","title":"Define Requirements","text":"<p>Defines a list of validation requirements for generated emails: require a salutation, enforce all-lowercase text via a validation function, and forbid mentioning \"purple elephants.\"</p> <pre><code># Add a requirements list that can be used for validation and repair exercises\nrequirements = [\n    req(\"The email should have a salutation\"),\n    req(\n        \"Use only lower-case letters\",\n        validation_fn=simple_validate(lambda x: x.lower() == x),\n    ),\n    check(\"Do not mention purple elephants.\"),\n]\n</code></pre>"},{"location":"lab_02/readme/#write-email-function","title":"Write email function","text":"<p>A function to generate a short email using the provided name and notes. This function instructs the Mellea session <code>m</code> to write an email using the global <code>requirements</code> list and a rejection-sampling strategy to try multiple candidates (loop_budget=5). If validation passes, the validated result is returned. If validation fails after sampling, the first sampled generation is returned as a fallback.</p> <p>The function takes 3 arguments</p> <ul> <li><code>m</code>: An active MelleaSession used to run the instruction.</li> <li><code>name</code>: Recipient name to be interpolated into the prompt.</li> <li><code>notes</code>: Notes to include in the body of the email.</li> </ul> <p>Returns:</p> <ul> <li>A string containing the generated email text.</li> </ul> <pre><code>def write_email(m: mellea.MelleaSession, name: str, notes: str) -&gt; str:\n\n    # Ask the model to write an email, passing validation requirements and\n    # a sampling strategy that will retry up to 5 times if candidates fail.\n    email_candidate = m.instruct(\n        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n        requirements=requirements,\n        strategy=RejectionSamplingStrategy(loop_budget=5),\n        user_variables={\"name\": name, \"notes\": notes},\n        return_sampling_results=True,\n    )\n\n    # If the instruction succeeded and a validated result is available,\n    # return the validated result (preferred).\n    if email_candidate.success:\n        return str(email_candidate.result)\n\n    # Otherwise, fall back to the first sampled generation (best-effort).\n    # This ensures the function always returns some text even if validation failed.\n    return email_candidate.sample_generations[0].value\n</code></pre>"},{"location":"lab_02/readme/#sample-usage","title":"Sample usage","text":"<pre><code># Create a Mellea model using the granite model and the ollama inference engine\nm = mellea.start_session()\n\n# Generate an email using the write_email function\nemail = write_email(\n    m,\n    \"Olivia\",\n    \"\"\"Olivia helped the lab over the last few weeks by organizing intern events, \n       advertising the speaker series, and handling issues with snack delivery.\"\"\",\n)\n\n# Display the generated email\ndisplay(Markdown(email))\n</code></pre>"},{"location":"lab_02/readme/#summary","title":"Summary","text":"<p>Most of this should look familiar by now, but the <code>validation_fn</code> and <code>check</code> should be new.</p> <p>We created 3 requirements:</p> <ul> <li>First requirement, the salutation, will be validated by LLM-as-a-judge on the output of the instruction. This is the default behavior.</li> <li>Second requirement, lower case, uses a function that takes the output of a sampling step and returns a boolean value indicating successful or unsuccessful validation. While the validation_fn parameter requires to run validation on the full session context, Mellea provides a wrapper for simpler validation functions (simple_validate(fn: Callable[[str], bool])) that take the output string and return a boolean as seen in this case.</li> <li>Third requirement is a <code>check()</code>. Checks are only used for validation, not for generation. Checks aim to avoid the \"do not think about B\" effect that often prime models (and humans) to do the opposite and \"think\" about B.</li> </ul> <p>We also saw in the <code>m = mellea.start_session()</code> how you can specify a different Ollama model, in case you want to try something other than Mellea's <code>ibm/granite4:micro</code> default.</p>"},{"location":"pre-work/readme/","title":"Pre-work","text":"<p>The labs in this workshop are Jupyter notebooks. Check out Running the Granite Notebooks section on how to setup the way you want to run the notebooks.</p> <ul> <li>Pre-work</li> <li>Running the Granite Notebooks Locally</li> <li>Local Prerequisites<ul> <li>Git</li> <li>Uv</li> </ul> </li> <li>Clone the Granite Workshop Repository<ul> <li>Sync the Python Virtual Environment</li> <li>Serving the Granite AI Models</li> <li>Replicate AI Cloud Platform</li> <li>Running Ollama Locally</li> </ul> </li> <li>Running the Granite Notebooks Remotely (Colab)<ul> <li>Colab Prerequisites</li> <li>Serving the Granite AI Models for Colab</li> <li>Replicate AI Cloud Platform for Colab</li> </ul> </li> </ul>"},{"location":"pre-work/readme/#running-the-granite-notebooks","title":"Running the Granite Notebooks","text":"<p>It is recommended if you want to run the lab notebooks locally on your computer that you have:</p> <ul> <li>A computer or laptop</li> <li>Knowledge of Git and Python</li> </ul> <p>Running the lab notebooks locally on your computer requires the following steps:</p>"},{"location":"pre-work/readme/#local-prerequisites","title":"Local Prerequisites","text":"<ul> <li>Git</li> <li>Uv</li> </ul>"},{"location":"pre-work/readme/#git","title":"Git","text":"<p>Git can be installed on most common operating systems like Windows,  Mac, and Linux. In fact, Git comes installed by default on most Mac and  Linux machines!</p> <p>For comprehensive instructions on how to install <code>git</code> on your laptop please refer to the Install Git page.</p> <p>To confirm the you have <code>git</code> installed correctly you can open a terminal window and type <code>git version</code>. You should receive a response like the one shown below.</p> <pre><code>git version\ngit version 2.39.5 (Apple Git-154)\n</code></pre>"},{"location":"pre-work/readme/#uv","title":"Uv","text":"<p><code>uv</code> is an extremely fast Python package and project manager, written in Rust.</p> <p>For detailed instructions on how to install <code>uv</code> on your laptop please refer to the Installing uv page where instructions for Mac, Windows and Linux machines can be found.</p> <p>To confirm the you have <code>uv</code> installed correctly you can open a terminal window and type <code>uv --version</code>. You should receive a response like the one shown below.</p> <pre><code>uv --version\nuv 0.6.12 (e4e03833f 2025-04-02)\n</code></pre>"},{"location":"pre-work/readme/#clone-the-granite-workshop-repository","title":"Clone the Granite Workshop Repository","text":"<p>Clone the workshop repository and cd into the repository directory.</p> <pre><code>git clone https://github.com/davidcolton/university_outreach_workshop\ncd university_outreach_workshop\n</code></pre>"},{"location":"pre-work/readme/#sync-the-python-virtual-environment","title":"Sync the Python Virtual Environment","text":"<p>The workshop repository uses a <code>pyproject.toml</code> file to define the version of Python to use and the required libraries to load. To sync your repository and setup Python and download the library dependancies run <code>uv sync</code> in a terminal. After syncing you have to activate your virtual environment.</p> <p>Note:</p> <p>If running on Windows it is suggested that you use the Windows Powershell running as administrator or, if you have it installed, the Windows Subsystem for Linux.</p> <pre><code>uv sync\n\n# Mac &amp; Linux\nsource .venv/bin/activate\n\n# Windows Powershell\n.venv\\Scripts\\activate\n</code></pre>"},{"location":"pre-work/readme/#running-ollama-locally","title":"Running Ollama Locally","text":"<p>If you want to run the AI models locally on your computer, you can use Ollama.</p> <p>Tested system</p> <p>This was tested on a Macbook with an M1 processor and 32GB RAM. It maybe possible to serve models with a CPU and less memory.</p> <p>Running Ollama locally on your computer requires the following steps:</p> <ol> <li> <p>Download and install Ollama, if you haven't already.</p> <p>On macOS, you can use Homebrew to install it with:</p> <pre><code>brew install ollama\n</code></pre> </li> <li> <p>Start the Ollama server. You will leave this running during the workshop.</p> <pre><code>ollama serve\n</code></pre> </li> <li> <p>In another terminal window, pull down the Granite models you will want to use in the workshop. Larger models take more memory to run but can give better results.</p> <pre><code>ollama pull granite4:micro\n</code></pre> </li> </ol>"},{"location":"pre-work/readme/#running-the-notebooks-remotely-colab","title":"Running the Notebooks Remotely (Colab)","text":"<p>If you are having difficulties getting your environment setup, or the workshop examples are not running successfully you can always run the Get Started with CoLab examples from the Mellea repository. Some of the content and examples may be different but you can still see the concepts shown here at work.</p> <p>Notebook execution speed tip</p> <p>The default execution runtime in Colab uses a CPU. Consider using a different Colab runtime to increase execution speed, especially in situations where you may have other constraints such as a slow network connection. From the navigation bar, select <code>Runtime -&gt; Change runtime type</code>, then select either GPU- or TPU-based hardware acceleration.</p>"},{"location":"pre-work/readme/#colab-prerequisites","title":"Colab Prerequisites","text":"<ul> <li>Google Colab requires a Google account that you're logged into</li> </ul>"}]}