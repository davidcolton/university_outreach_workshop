{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IBM University Outreach Workshop","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to our workshop! In this workshop we'll be using the open-sourced IBM Granite AI foundation models and Mellea for a number of use cases that demonstrates the value of generative AI.</p> <p>By the end of this workshop, you will be able to:</p> <ul> <li>Run a simple \"Hello Mellea\" program</li> <li>Use Mellea to Instruct, Validate, Repair</li> <li>Have some fun with <code>@generative</code> functions</li> <li>Use Mellea and Docling</li> <li>...</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Introduction</li> <li>About this workshop</li> <li>Agenda</li> <li>Technology Used</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Link to Content Description Lab 0: Pre-work Pre-work for the workshop Lab 1: Hello Mellea Lab 2: Instruct-Validate-Repair Lab 3: Writing Compositional Code with Generative Stubs Lab 4: Docling and Mellea Lab 5: Generative Objects Lab 6: Information Extraction with Generative Slots Lab 7: MiniResearcher: A Retrieval-Augmented Generative Pipeline"},{"location":"#technology-used","title":"Technology Used","text":"<p>The technology used in the workshop is as follows:</p> <ul> <li>IBM Granite AI foundation models](https://www.ibm.com/granite)</li> <li>Mellea</li> <li>Docling</li> <li>Jupyter notebooks</li> <li>LangChain</li> <li>Ollama</li> </ul>"},{"location":"labs/lab_01/","title":"What is Generative Computing?","text":"<p>A generative program is any computer program that contains calls to an LLM. As we will see throughout the documentation, LLMs can be incorporated into software in a wide variety of ways. Some ways of incorporating LLMs into programs tend to result in robust and performant systems, while others result in software that is brittle and error-prone.</p> <p>Generative programs are distinguished from classical programs by their use of functions that invoke generative models. These generative calls can produce many different data types \u2014 strings, booleans, structured data, code, images/video, and so on. The model(s) and software underlying generative calls can be combined and composed in certain situations and in certain ways (e.g., LoRA adapters as a special case). In addition to invoking generative calls, generative programs can invoke other functions, written in languages that do not have an LLM in their base, so that we can, for example, pass the output of a generative function into a DB retrieval system and feed the output of that into another generator. Writing generative programs is difficult because generative programs interleave deterministic and stochastic operations.</p> <p>If you would like to read more about this, please don't hesitate to take a look here.</p>"},{"location":"labs/lab_01/#mellea","title":"Mellea","text":"<p>Mellea is a library for writing generative programs. Generative programming replaces flaky agents and brittle prompts with structured, maintainable, robust, and efficient AI workflows.</p>"},{"location":"labs/lab_01/#features","title":"Features","text":"<ul> <li>A standard library of opinionated prompting patterns.</li> <li>Sampling strategies for inference-time scaling.</li> <li>Clean integration between verifiers and samplers.</li> <li>Batteries-included library of verifiers.</li> <li>Support for efficient checking of specialized requirements using activated LoRAs.</li> <li>Train your own verifiers on proprietary classifier data.</li> <li>Compatible with many inference services and model families. Control cost and quality by easily lifting and shifting workloads between:</li> <li>inference providers</li> <li>model families</li> <li>model sizes</li> <li>Easily integrate the power of LLMs into legacy code-bases (mify).</li> <li>Sketch applications by writing specifications and letting <code>mellea</code> fill in the details (generative slots).</li> <li>Get started by decomposing your large unwieldy prompts into structured and maintainable Mellea problems.</li> </ul>"},{"location":"labs/lab_01/#lab-1-hello-mellea","title":"Lab 1: Hello Mellea","text":"<p>Running <code>mellea.start_session()</code> initialize a new <code>MelleaSession</code>. The session holds three things:</p> <ol> <li>The model to use for this session.</li> <li>An inference engine; i.e., the code that actually calls our model. We will be using ollama, but you can also use Huggingface or any OpenAI-compatible endpoint.</li> <li>A <code>Context</code>, which tells Mellea how to remember context between requests. This is sometimes called the \"Message History\" in other frameworks. Throughout these early tutoriala, we will be using a <code>SimpleContext</code>. In <code>SimpleContext</code>, every request starts with a fresh context. There is no preserved chat history between requests. Mellea provides other types of context, but for now we will not be using those features. See the Mellea Tutorials for further details.</li> </ol>"},{"location":"labs/lab_01/#prerequisites","title":"Prerequisites","text":"<p>This lab is a Jupyter notebook. Please follow the instructions in pre-work to configure the environment for the lab.</p>"},{"location":"labs/lab_01/#loading-the-lab","title":"Loading the Lab","text":"<p>To run the notebook from command line in Jupyter using the activated virtual environment from the pre-work, run:</p> <pre><code>jupyter-lab\n</code></pre> <p>When Jupyter Lab opens the path to the <code>labs/lab_01.ipynb</code> notebook file is relative to the root folder from the git clone in the  pre-work. The folder navigation pane on the left-hand side can be used to navigate to the file. Once the notebook has been found it can be double clicked and it will open to the pane on the right.</p> <p>Alternatively, if you already have it configured and setup on you laptop, VSCode could be used either (don't forget to select the virtual environment).</p> <p>Note</p> <p>All notebooks in this set of examples can be opened in this manner apart from <code>lab_07</code> which is a python script and is run from the command line.</p>"},{"location":"labs/lab_01/#imports","title":"Imports","text":"<pre><code># Import necessary libraries\nimport mellea\n\n# Display utilities\nfrom IPython.display import display, Markdown\n\n# Format code cells with black\nimport jupyter_black\njupyter_black.load() \n</code></pre> <p>To use Mellea we import the <code>mellea</code> library. The other imports are only for displaying output and for formatting the notebook cells and can be ignored.</p> <p>Note</p> <p>If you see something about the Rust compiler, please confirm you are using python3.11, or python3.12 anything above that has a Rust dependency.</p>"},{"location":"labs/lab_01/#chat-with-mellea","title":"Chat with Mellea","text":"<p>We can have a simple chat with a <code>mellea</code> session by starting a session, <code>m</code>, and then using the <code>chat</code> function. We can then display the answer by displaying the answer objects content.</p> <pre><code># Create a Mellea model using the granite3.3:8b model and the ollama inference engine\nm = mellea.start_session()\n\n# Send a chat message to the model\n# In this example, we are asking for fun trivia about IBM and the early history of AI.\n# Since we are using a SimpleContext, there is no preserved chat history between requests.\n# Each request starts fresh.\nanswer = m.chat(\n    \"tell me some fun trivia about IBM and the early history of AI.\"\n)\n\n# Display the answer in markdown format\ndisplay(Markdown(answer.content))\n</code></pre>"},{"location":"labs/lab_01/#start-a-session-with-linearcontext","title":"Start a Session with LinearContext","text":"<p>In the first example we have used SimpleContext, a context manager that resets the chat message history on each model call. That is, the model's context is entirely determined by the current Component.</p> <p>Mellea also provides a LinearContext, which behaves like a chat history. We can use the ChatContext to interact with chat models</p> <pre><code>from mellea import start_session\nfrom mellea.stdlib.base import ChatContext\n\n# Create a session with chat context\nm = start_session(ctx=ChatContext())\n\n# Generate and store responses\nproblem = m.chat(\"Make up a math problem.\")\nsolution = m.chat(\"Solve your math problem.\")\n\n# Display the problem and solution using the content attribute\ndisplay(Markdown(f\"Problem:\\n{problem.content}\\nSolution:\\n{solution.content}\"))\n</code></pre>"},{"location":"labs/lab_02/","title":"Lab 2: Instruct-Validate-Repair","text":"<p>Instruct-Validate-Repair is a design pattern for building robust automation using LLMs. The idea is simple:</p> <ol> <li>Instruct the model to perform a task and specify requirements on the output of the task.</li> <li>Validate that these requirements are satisfied by the model's output.</li> <li>If any requirements fail, try to repair.</li> </ol>"},{"location":"labs/lab_02/#imports","title":"Imports","text":"<p>In addition to importing <code>mellea</code> from <code>stdlib.requirements</code> we import <code>check</code>, <code>req</code> and <code>simple_validate</code>. These provide syntactic sugar for writing validation functions that operate over the last output from the model (interpreted as a string).</p> <p>This is useful when your validation logic only depends upon the most recent model output.</p> <pre><code># Import necessary libraries\nimport mellea\nfrom mellea.stdlib.requirement import check, req, simple_validate\nfrom mellea.stdlib.sampling import RejectionSamplingStrategy\n</code></pre>"},{"location":"labs/lab_02/#define-requirements","title":"Define Requirements","text":"<p>Defines a list of validation requirements for generated emails: require a salutation, enforce all-lowercase text via a validation function, and forbid mentioning \"purple elephants.\"</p> <pre><code># Add a requirements list that can be used for validation and repair exercises\nrequirements = [\n    req(\"The email should have a salutation\"),\n    req(\n        \"Use only lower-case letters\",\n        validation_fn=simple_validate(lambda x: x.lower() == x),\n    ),\n    check(\"Do not mention purple elephants.\"),\n]\n</code></pre>"},{"location":"labs/lab_02/#write-email-function","title":"Write email function","text":"<p>A function to generate a short email using the provided name and notes. This function instructs the Mellea session <code>m</code> to write an email using the global <code>requirements</code> list and a rejection-sampling strategy to try multiple candidates (loop_budget=5). If validation passes, the validated result is returned. If validation fails after sampling, the first sampled generation is returned as a fallback.</p> <p>The function takes 3 arguments</p> <ul> <li><code>m</code>: An active MelleaSession used to run the instruction.</li> <li><code>name</code>: Recipient name to be interpolated into the prompt.</li> <li><code>notes</code>: Notes to include in the body of the email.</li> </ul> <p>Returns:</p> <ul> <li>A string containing the generated email text.</li> </ul> <pre><code>def write_email(m: mellea.MelleaSession, name: str, notes: str) -&gt; str:\n\n    # Ask the model to write an email, passing validation requirements and\n    # a sampling strategy that will retry up to 5 times if candidates fail.\n    email_candidate = m.instruct(\n        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n        requirements=requirements,\n        strategy=RejectionSamplingStrategy(loop_budget=5),\n        user_variables={\"name\": name, \"notes\": notes},\n        return_sampling_results=True,\n    )\n\n    # If the instruction succeeded and a validated result is available,\n    # return the validated result (preferred).\n    if email_candidate.success:\n        return str(email_candidate.result)\n\n    # Otherwise, fall back to the first sampled generation (best-effort).\n    # This ensures the function always returns some text even if validation failed.\n    return email_candidate.sample_generations[0].value\n</code></pre>"},{"location":"labs/lab_02/#sample-usage","title":"Sample usage","text":"<pre><code># Create a Mellea model using the granite model and the ollama inference engine\nm = mellea.start_session()\n\n# Generate an email using the write_email function\nemail = write_email(\n    m,\n    \"Olivia\",\n    \"\"\"Olivia helped the lab over the last few weeks by organizing intern events, \n       advertising the speaker series, and handling issues with snack delivery.\"\"\",\n)\n\n# Display the generated email\ndisplay(Markdown(email))\n</code></pre>"},{"location":"labs/lab_02/#summary","title":"Summary","text":"<p>Most of this should look familiar by now, but the <code>validation_fn</code> and <code>check</code> should be new.</p> <p>We created 3 requirements:</p> <ul> <li>First requirement, the salutation, will be validated by LLM-as-a-judge on the output of the instruction. This is the default behavior.</li> <li>Second requirement, lower case, uses a function that takes the output of a sampling step and returns a boolean value indicating successful or unsuccessful validation. While the validation_fn parameter requires to run validation on the full session context, Mellea provides a wrapper for simpler validation functions (simple_validate(fn: Callable[[str], bool])) that take the output string and return a boolean as seen in this case.</li> <li>Third requirement is a <code>check()</code>. Checks are only used for validation, not for generation. Checks aim to avoid the \"do not think about B\" effect that often prime models (and humans) to do the opposite and \"think\" about B.</li> </ul> <p>We also saw in the <code>m = mellea.start_session()</code> how you can specify a different Ollama model, in case you want to try something other than Mellea's <code>ibm/granite4:micro</code> default.</p>"},{"location":"labs/lab_03/","title":"Lab 3 \u2014 Writing Compositional Code with Generative Stubs","text":"<p>This lab demonstrates how to use Mellea's <code>@generative</code> slots as compositional building blocks. It shows a small summarizer library, decision-aide functions, and compositionality checks that let LLM-powered functions be combined in predictable, testable ways.</p> <p>In classical programming, pure (stateless) functions are a simple and powerful abstraction. A pure function takes inputs, computes outputs, and has no side effects. Generative programs can also use functions as abstraction boundaries, but in a generative program the meaning of the function can be given by an LLM instead of an interpreter or compiler. This is the idea behind a GenerativeSlot.</p> <p>A GenerativeSlot is a function whose implementation is provided by an LLM. In Mellea, you define these using the <code>@generative</code> decorator. The function signature specifies the interface, and the docstring (or type annotations) guides the LLM in producing the output.</p> <p>In this lab, we will see how compositionality checks can be used to combine libraries of generative functions.</p>"},{"location":"labs/lab_03/#goals","title":"Goals","text":"<ul> <li>Show how to define generative functions (GenerativeSlots) with clear signatures and docstrings.</li> <li>Build small, composable libraries: summarizers and decision aides.</li> <li>Use compositionality checks to gate downstream decisions.</li> <li>Demonstrate wiring these pieces together on a meeting transcript.</li> </ul>"},{"location":"labs/lab_03/#main-code-snippets-of-interest","title":"Main code snippets of interest","text":""},{"location":"labs/lab_03/#1-imports-and-setup","title":"1. Imports and setup","text":"<p>The notebook initialises Mellea and type hints. This cell sets the execution environment used by all generative calls.</p> <p>Key items: - <code>from mellea import generative</code> - <code>from typing import Literal</code></p>"},{"location":"labs/lab_03/#2-summariser-library","title":"2. Summariser library","text":"<p>A small collection of generative slots that produce human-readable summaries.</p> <p>Representative function signatures: <pre><code>@generative\ndef summarize_meeting(transcript: str) -&gt; str: ...\n\n@generative\ndef summarize_contract(contract_text: str) -&gt; str: ...\n\n@generative\ndef summarize_short_story(story: str) -&gt; str: ...\n</code></pre></p> <p>Each function's docstring guides the LLM to produce outputs matching the annotated return type (here, <code>str</code>). Please view the lab notebook to see the full docstring.</p> <p>Note:    <code>@generative</code>: Converts a function into an AI-powered function. This decorator transforms a regular Python function into one that uses an LLM to generate outputs. The function's entire signature - including its name, parameters, docstring, and type hints - is used to instruct the LLM to imitate that function's behavior. The output is guaranteed to match the return type annotation using structured outputs and automatic validation.</p>"},{"location":"labs/lab_03/#3-decision-aides-library","title":"3. Decision-aides library","text":"<p>Generative slots that turn structured summaries into actionable recommendations or mitigations.</p> <p>Representative functions:</p> <pre><code>@generative\ndef propose_business_decision(summary: str) -&gt; str: ...\n\n@generative\ndef generate_risk_mitigation(summary: str) -&gt; str: ...\n\n@generative\ndef generate_novel_recommendations(summary: str) -&gt; str: ...\n</code></pre> <p>These are intended to be called only when upstream compositionality checks validate the summary.</p>"},{"location":"labs/lab_03/#4-example-summarize-a-meeting-transcript","title":"4. Example: Summarize a meeting transcript","text":"<p>A long example transcript is provided and passed to the summarizer:</p> <pre><code>m = mellea.start_session()\nsummary = summarize_meeting(m, transcript=transcript)\ndisplay(Markdown(f\"# Summary of meeting:\\n {summary}\"))\n</code></pre> <p>This demonstrates end-to-end use of a generative slot and rendering results in a notebook.</p>"},{"location":"labs/lab_03/#5-compositionality-checks","title":"5. Compositionality checks","text":"<p>Small generative validators that return <code>Literal[\"yes\",\"no\"]</code> and are used to control flow:</p> <ul> <li><code>has_structured_conclusion(summary: str) -&gt; Literal[\"yes\",\"no\"]</code></li> <li><code>contains_actionable_risks(summary: str) -&gt; Literal[\"yes\",\"no\"]</code></li> <li><code>has_theme_and_plot(summary: str) -&gt; Literal[\"yes\",\"no\"]</code></li> </ul> <p>These functions let the program ask LLMs whether an output satisfies a structural property before invoking downstream decision aides.</p>"},{"location":"labs/lab_03/#6-applying-decision-aides-conditionally","title":"6. Applying decision aides conditionally","text":"<p>Example control flow that shows how compositionality checks gate downstream actions:</p> <pre><code>if contains_actionable_risks(m, summary=summary) == \"yes\":\n    mitigation = generate_risk_mitigation(m, summary=summary)\n    display(Markdown(f\"## Mitigation:\\n{mitigation}\"))\nelse:\n    display(Markdown(\"Summary does not contain actionable risks.\"))\n</code></pre> <p>and</p> <pre><code>if has_structured_conclusion(m, summary=summary) == \"yes\":\n    decision = propose_business_decision(m, summary=summary)\n    display(Markdown(f\"# Decision:\\n{decision}\"))\nelse:\n    display(Markdown(\"Summary lacks a structured conclusion.\"))\n</code></pre>"},{"location":"labs/lab_03/#notes-and-best-practices","title":"Notes and best practices","text":"<ul> <li>Keep generative slot docstrings precise: the function signature + docstring form the spec the LLM imitates.</li> <li>Prefer machine-friendly return types (lists, dicts, typed literals) to simplify downstream aggregation and validation.</li> <li>Use compositionality checks to reduce downstream hallucination and to make control flow auditable.</li> </ul>"},{"location":"labs/lab_04/","title":"Lab 4 \u2014 Docling and Mellea","text":"<p>This lab demonstrates how to combine Docling (document extraction) with Mellea (generative workflows) to extract structured content from PDFs and perform LLM-powered transformations on extracted objects.</p> <p>Goals</p> <ul> <li>Load a PDF into a RichDocument using Docling.</li> <li>Extract table objects and inspect them.</li> <li>Use a Mellea session (with an Ollama backend) to transform Docling Table objects.</li> <li>Observe how Mellea-friendly Docling objects can be used directly with LLM transforms.</li> </ul>"},{"location":"labs/lab_04/#1-setup-and-imports","title":"1. Setup and imports","text":"<p>Core imports for Docling, Mellea, the Ollama backend, and notebook display utilities.</p> <pre><code># Python imports for [lab_04.ipynb](http://_vscodecontentref_/0)\nfrom pathlib import Path\nimport mellea\nfrom mellea.backends.model_ids import META_LLAMA_3_2_3B\nfrom mellea.backends.ollama import OllamaModelBackend\nfrom mellea.backends.types import ModelOption\nfrom mellea.stdlib.docs.richdocument import RichDocument\nfrom mellea.stdlib.docs.richdocument import Table\n</code></pre>"},{"location":"labs/lab_04/#2-load-a-pdf-and-extract-the-first-table","title":"2. Load a PDF and extract the first table","text":"<p>This snippet uses RichDocument.from_document_file to load a PDF (via URL or file path), then gets the first table and renders it as Markdown.</p> <pre><code># Load a document from a URL and extract the first table\nrd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")\n\n# Extract the first table from the document\nfirst_table: Table = rd.get_tables()[0]\n\n# Display the first table in markdown format\ndisplay(Markdown(first_table.to_markdown()))\n</code></pre> <p>Why it matters: RichDocument wraps Docling-extracted structure (text, tables, figures) into Mellea-friendly objects that can be inspected or passed to transforms.</p>"},{"location":"labs/lab_04/#3-transform-a-table-with-a-mellea-session-and-ollama-backend","title":"3. Transform a Table with a Mellea session and Ollama backend","text":"<p>Demonstrates creating a MelleaSession configured for a specific model and calling transform repeatedly with different random seeds to see variability in LLM outputs. The transform instruction asks the model to add a \"Model\" column derived from the \"Feature\" column.</p> <pre><code># Create a Mellea session using an Ollama backend model\nm_llama = mellea.MelleaSession(backend=OllamaModelBackend(model_id=META_LLAMA_3_2_3B))\n\nfor seed in [x * 12 for x in range(5)]:\n    table2 = m_llama.transform(\n        first_table,\n        \"Add a 'Model' column as the last column that extracts which model was used for that feature or 'None' if none.\",\n        model_options={ModelOption.SEED: seed},\n    )\n    if isinstance(table2, Table):\n        display(Markdown(table2.to_markdown()))\n    else:\n        print(\"==== TRYING AGAIN after non-useful output.====\")\n</code></pre> <p>Notes:</p> <ul> <li>model_options can control behavior (e.g., deterministic sampling via seed).</li> <li>The transform returns a new Table object when the LLM output conforms; otherwise you should handle retries or fallbacks.</li> </ul>"},{"location":"labs/lab_04/#4-working-with-the-table-object","title":"4. Working with the Table object","text":"<p>The Table returned by RichDocument / transforms is an object with methods to export, inspect, or feed into other generative slots. Typical operations include converting to markdown, CSV, or passing into further transforms. Here we just connect to Markdown to display the table.</p> <pre><code># Render table as markdown for human inspection\ndisplay(Markdown(first_table.to_markdown()))\n</code></pre>"},{"location":"labs/lab_04/#tips-best-practices","title":"Tips &amp; best practices","text":"<ul> <li>Keep transform instructions precise and scoped; ask for structured outputs when you expect objects back.</li> <li>Inspect the RichDocument structure before transforming (rd.get_tables(), rd.get_figures(), etc.).</li> <li>Use model options to control sampling behavior when debugging nondeterministic outputs.</li> </ul>"},{"location":"labs/lab_05/","title":"Lab 5 \u2014 Generative Objects","text":"<p>This lab demonstrates how to wrap structured data with domain methods and expose them to LLMs using Mellea's MObject pattern. You will learn how to convert a plain Python class into an MObject with <code>@mify</code>, parse CSV-style text into a DataFrame, and run LLM-driven queries and transforms against the object.</p>"},{"location":"labs/lab_05/#goals","title":"Goals","text":"<ul> <li>Introduce the MObject pattern and <code>@mify</code>.</li> <li>Show how to store structured data (CSV string) with helper methods.</li> <li>Demonstrate using a Mellea session to query and transform the object.</li> </ul>"},{"location":"labs/lab_05/#1-setup-imports","title":"1. Setup &amp; imports","text":"<p>Key imports required for this lab.</p> <pre><code># Python imports for \nimport pandas as pd\nimport mellea\nfrom mellea.stdlib.mify import mify\n</code></pre>"},{"location":"labs/lab_05/#2-mycompanydatabase-mobject","title":"2. MyCompanyDatabase MObject","text":"<p>This class holds a CSV table as a string and exposes parsing and update helpers. The <code>@mify</code> decorator controls what is visible to the LLM and how the object is rendered.</p> <pre><code>@mify(fields_include={\"table\"}, template=\"{{ table }}\")\nclass MyCompanyDatabase:\n    def __init__(self, *, table: str | None = None):\n        if table is not None:\n            self.table = table\n\n    def _parse_table(self, table: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Parse the CSV table string into a pandas DataFrame and coerce Sales to int.\n        \"\"\"\n        df = pd.read_csv(StringIO(table), sep=\",\")\n        if \"Sales\" in df.columns:\n            df[\"Sales\"] = pd.to_numeric(\n              df[\"Sales\"].astype(str).str.strip(), errors=\"coerce\").fillna(0).astype(int)\n        return df\n\n    def update_sales(self, store: str, amount: int) -&gt; \"MyCompanyDatabase\":\n        \"\"\"\n        Update the sales for `store` to `amount` and return a new MyCompanyDatabase.\n        \"\"\"\n        df = self._parse_table(self.table)\n        mask = df[\"Store\"].astype(str).str.strip().str.lower() == store.strip().lower()\n        df.loc[mask, \"Sales\"] = int(amount)\n        return MyCompanyDatabase(table=df.to_csv(sep=\",\", index=False, header=True))\n</code></pre> <p>Notes:</p> <ul> <li><code>_parse_table</code> ensures the Sales column is numeric.</li> <li><code>update_sales</code> returns a new instance (functional/immutable style) suitable for generative transformations.</li> </ul>"},{"location":"labs/lab_05/#3-example-usage-query-transform","title":"3. Example usage: query &amp; transform","text":"<p>Create a Mellea session, query the object, and apply a transform to update it.</p> <pre><code># Create a Mellea session\nm = mellea.start_session()\n\n# Construct the database and ask a question\ndb = MyCompanyDatabase()\nprint(m.query(db, \"What were sales for the Northeast branch this month?\"))\n\n# Apply a transform (LLM-driven) to update data\ndb = m.transform(db, \"Update the northeast sales to 1250.\")\nprint(m.query(db, \"What were sales for the Northeast branch this month?\"))\n</code></pre> <p>Tips:</p> <ul> <li>Use <code>m.query</code> for read-only LLM queries and <code>m.transform</code> to request object updates.</li> <li>Keep transformation instructions narrow and structured so the LLM returns a valid object.</li> </ul>"},{"location":"labs/lab_05/#best-practices","title":"Best practices","text":"<ul> <li>Prefer machine-friendly return types and well-documented methods; docstrings become the LLM's spec.</li> <li>Expose only necessary fields/methods to the LLM via <code>@mify(fields_include=...)</code>.</li> <li>Use small, testable transforms and explicit validation to reduce hallucination and keep pipelines auditable.</li> </ul>"},{"location":"labs/lab_06/","title":"Lab 6 \u2014 Information Extraction with Generative Slots","text":"<p>This lab demonstrates simple information extraction using Mellea generative slots. You will define a typed generative function to extract person names from text, run it against a sample document, and inspect the structured output.</p>"},{"location":"labs/lab_06/#goals","title":"Goals","text":"<ul> <li>Introduce a generative slot for information extraction.</li> <li>Show how to start a Mellea session and call generative functions.</li> <li>Provide a small, reproducible example for extracting person names from text.</li> </ul>"},{"location":"labs/lab_06/#1-setup-imports","title":"1. Setup &amp; imports","text":"<p>Use these imports to create the session and display results.</p> <pre><code># Import necessary libraries and display utilities\nfrom mellea import generative, start_session\nfrom mellea.backends import model_ids\n\nfrom IPython.display import display, Markdown\n</code></pre>"},{"location":"labs/lab_06/#2-define-the-generative-extractor","title":"2. Define the generative extractor","text":"<p>This generative slot defines the interface and docstring the LLM will follow. The return type is a list of strings (person names).</p> <pre><code># Define a generative function to extract person names\n@generative\ndef extract_all_person_names(doc: str) -&gt; list[str]:\n    \"\"\"\n    Extract all person names mentioned in the given document text.\n\n    This function analyzes the input text and identifies names of people,\n    including full names, titles, and honorifics. It returns a list of unique\n    person names found in the document.\n\n    Args:\n        doc (str): The input document text to analyze for person names.\n                   Can contain paragraphs, quotes, or any text format.\n\n    Returns:\n        list[str]: A list of strings where each string is a person's name.\n                   Names may include titles (e.g., \"President Obama\", \"Dr. Smith\").\n                   Returns an empty list if no names are found.\n    \"\"\"\n</code></pre>"},{"location":"labs/lab_06/#3-example-usage","title":"3. Example usage","text":"<p>Start a session, provide sample text, and call the generative extractor. The example shows how to pass the session and input and how to print results.</p> <pre><code># Start a Mellea session\nm = start_session()\n\n# Example document text\nNYTimes_text = \"\"\"CAMP DAVID, Md. \u2014 Leaders of the world's richest countries banded together on Saturday\nto press Germany to back more pro-growth policies ... as President Obama ... cannot afford Chancellor Angela Merkel's ...\"\"\"\n\n# Call the generative extractor\nperson_names = extract_all_person_names(m, doc=NYTimes_text)\n\n# Display results\nprint(f\"person_names = {person_names}\")\n# expected: person_names = ['President Obama', 'Angela Merkel']\n</code></pre>"},{"location":"labs/lab_06/#4-notes-and-tips","title":"4. Notes and tips","text":"<ul> <li>Keep the function signature and docstring precise; they form the spec the LLM imitates.</li> <li>Use typed return values (lists, dicts, literals) to simplify validation and downstream processing.</li> <li>Test on varied documents (news, papers, transcripts) to evaluate extraction robustness.</li> </ul>"},{"location":"labs/lab_06/#5-exercises","title":"5. Exercises","text":"<ul> <li>Extend the extractor to include entity types (person, organization, location) and return structured records.</li> <li>Add a confidence field or provenance (text span) for each extracted name.</li> <li>Compose the extractor with a deduplication/normalization step (e.g., convert \"B. Obama\" \u2192 \"Barack Obama\").</li> </ul>"},{"location":"labs/lab_07/","title":"Lab 07: MiniResearcher Pipeline","text":""},{"location":"labs/lab_07/#introduction","title":"Introduction","text":"<p>This lab demonstrates a structured generative programming pipeline using the Mellea framework and Ollama model backend. The goal is to generate a professional report based on a subtopic and main topic using a set of source documents. The pipeline includes safety validation, summarization, outline generation, and final report synthesis. It leverages Mellea's <code>MelleaSession</code> for model interaction, <code>Requirement</code> objects for constraint validation, and <code>RejectionSamplingStrategy</code> to ensure outputs meet defined standards.</p> <p>The main highlights are given below.</p>"},{"location":"labs/lab_07/#session-initialization","title":"Session Initialization","text":"<pre><code>@cache\ndef get_session():\n    return MelleaSession(backend=OllamaModelBackend(model_ids.IBM_GRANITE_4_MICRO_3B))\n\n@cache\ndef get_guardian_session():\n    return MelleaSession(\n        backend=OllamaModelBackend(model_ids.IBM_GRANITE_GUARDIAN_3_0_2B)\n    )\n</code></pre> <p>These functions initialize and cache two model sessions:</p> <ul> <li><code>get_session()</code> sets up the main model used for summarization and report generation.</li> <li><code>get_guardian_session()</code> initializes a safety-focused model to validate document content.</li> </ul> <p>Highlights:</p> <ul> <li>Uses <code>OllamaModelBackend</code> to run local LLMs.</li> <li>Caching avoids repeated initialization.</li> <li>Supports modular backend switching.</li> </ul>"},{"location":"labs/lab_07/#input-safety-check","title":"Input Safety Check","text":"<pre><code>def step_is_input_safe(guardian_session: MelleaSession, docs: list[str]) -&gt; bool:\n    is_safe = True\n    for i_doc, doc in enumerate(docs):\n        inspect = guardian_session.chat(doc)\n        if str(inspect).upper().startswith(\"YES\"):\n            is_safe = False\n            break\n    return is_safe\n</code></pre> <p>This function checks each document for harmful content using the guardian model.</p> <p>Highlights:</p> <ul> <li>Iterates through all documents.</li> <li>Uses <code>chat()</code> to query the guardian model.</li> <li>Flags unsafe content based on model response.</li> </ul>"},{"location":"labs/lab_07/#document-summarization","title":"Document Summarization","text":"<pre><code>def step_summarize_docs(s: MelleaSession, docs: list[str], user_args: dict) -&gt; list[str]:\n    summaries = []\n    for doc in docs:\n        summary = s.instruct(\n            f\"Summarize the following document to answer the question: 'How {{current_subtopic}} impacts {{main_topic}}?' \n Document: {doc}\",\n            requirements=[\"Use maximal 3 sentences.\"],\n            user_variables=user_args,\n        )\n        summaries.append(str(summary))\n    return summaries\n</code></pre> <p>This function summarizes each document with a task-specific prompt.</p> <p>Highlights:</p> <ul> <li>Uses <code>instruct()</code> to generate summaries.</li> <li>Applies a sentence limit constraint.</li> <li>Injects user-defined variables for context.</li> </ul>"},{"location":"labs/lab_07/#outline-generation","title":"Outline Generation","text":"<pre><code>def step_generate_outline(s: MelleaSession, user_args: dict, context: list[RAGDocument]) -&gt; list[str]:\n    class SectionTitles(BaseModel):\n        section_titles: list[str]\n\n    def must_have_sections(out: str) -&gt; bool:\n        stt = SectionTitles.model_validate_json(out)\n        return is_a_true_subset_of_b([\"Introduction\", \"Conclusion\", \"References\"], stt.section_titles)\n\n    def max_sub_sections(out: str) -&gt; bool:\n        stt = SectionTitles.model_validate_json(out)\n        return len(stt.section_titles) &lt;= 3 + user_args[\"max_subsections\"]\n\n    req_outline = Requirement(\"Include Introduction, Conclusion, and References\", validation_fn=simple_validate(must_have_sections))\n    req_num_sections = Requirement(f\"Limit subsections to {user_args['max_subsections']}\", validation_fn=simple_validate(max_sub_sections))\n\n    outline_result = s.instruct(\n        description=\"Create an outline for a report...\",\n        requirements=[req_outline, req_num_sections],\n        user_variables=user_args,\n        grounding_context={f\"Document {i+1}\": f\"## Title: {d.title}, ## Source: {d.source}\" for i, d in enumerate(context)},\n        strategy=RejectionSamplingStrategy(loop_budget=2),\n        return_sampling_results=True,\n        format=SectionTitles,\n    )\n    return SectionTitles.model_validate_json(outline_result.value).section_titles\n</code></pre> <p>This function generates a structured outline for the report.</p> <p>Highlights:</p> <ul> <li>Uses Pydantic <code>BaseModel</code> for structured output.</li> <li>Validates outline using <code>Requirement</code> and <code>simple_validate</code>.</li> <li>Applies <code>RejectionSamplingStrategy</code> to enforce constraints.</li> </ul>"},{"location":"labs/lab_07/#report-writing","title":"Report Writing","text":"<pre><code>def step_write_full_report(m: MelleaSession, max_words: int, user_args: dict, summaries: list[str], outline: list[str]) -&gt; str:\n    req_focus = Requirement(\"Stay focused on the topic.\")\n    req_language = Requirement(f\"Write in {user_args['language']}.\")\n    req_tone = Requirement(\"Use a {{tone}} tone.\")\n    req_length = Requirement(\n        f\"Max length {max_words} words.\",\n        validation_fn=simple_validate(create_check_word_count(max_words))\n    )\n\n    user_args.update({\n        \"context\": \"\n\".join(summaries),\n        \"outline\": \"\n\".join([f\"* {o}\" for o in outline]),\n    })\n\n    report_result = m.instruct(\n        description=\"Context:\n{{context}}\nSummarize into a detailed report...\",\n        requirements=[req_focus, req_length, req_language, req_tone],\n        user_variables=user_args,\n        strategy=RejectionSamplingStrategy(loop_budget=2, requirements=[req_length]),\n        return_sampling_results=True,\n    )\n    return report_result.value\n</code></pre> <p>This function composes the final report using summaries and outline.</p> <p>Highlights:</p> <ul> <li>Enforces multiple constraints: tone, language, length.</li> <li>Uses <code>RejectionSamplingStrategy</code> to retry until valid output.</li> <li>Combines structured context and outline for coherent generation.</li> </ul>"},{"location":"labs/lab_07/#lab_07_contextpy","title":"<code>lab_07_context.py</code>","text":"<p>This file defines the <code>RAGDocument</code> dataclass and provides a list of documents used in the pipeline.</p> <pre><code>@dataclass\nclass RAGDocument:\n    title: str\n    source: str\n    content: str\n</code></pre> <p>Highlights:</p> <ul> <li>Encapsulates metadata and content.</li> <li>Used for grounding context in outline generation.</li> <li>Enables structured document handling.</li> </ul>"},{"location":"labs/lab_07/#how-to-run-the-script","title":"How to Run the Script","text":"<ol> <li>Install dependencies (if run in Workshop folder and virtual environment is activated this is not required as dependencies are already resolved):</li> </ol> <pre><code>pip install mellea openai pydantic\n</code></pre> <ol> <li>Run the script:</li> </ol> <pre><code>python lab_07.py\n</code></pre> <ol> <li> <p>Output includes:</p> </li> <li> <p>Safety validation results</p> </li> <li>Summaries of documents</li> <li>Generated outline</li> <li>Final report</li> </ol>"},{"location":"labs/pre_work/","title":"Pre-work","text":"<p>The labs in this workshop are Jupyter notebooks. Check out Running the Granite Notebooks section on how to setup the way you want to run the notebooks.</p> <ul> <li>Pre-work</li> <li>Local Prerequisites<ul> <li>Git</li> <li>Uv</li> </ul> </li> <li>Clone the Granite Workshop Repository<ul> <li>Sync the Python Virtual Environment</li> <li>Running Ollama Locally</li> </ul> </li> <li>Running the Notebooks Remotely (Colab)</li> </ul>"},{"location":"labs/pre_work/#running-the-granite-notebooks","title":"Running the Granite Notebooks","text":"<p>It is recommended if you want to run the lab notebooks locally on your computer that you have:</p> <ul> <li>A computer or laptop</li> <li>Knowledge of Git and Python</li> </ul> <p>Running the lab notebooks locally on your computer requires the following steps:</p>"},{"location":"labs/pre_work/#local-prerequisites","title":"Local Prerequisites","text":"<ul> <li>Git</li> <li>Uv</li> </ul>"},{"location":"labs/pre_work/#git","title":"Git","text":"<p>Git can be installed on most common operating systems like Windows,  Mac, and Linux. In fact, Git comes installed by default on most Mac and  Linux machines!</p> <p>For comprehensive instructions on how to install <code>git</code> on your laptop please refer to the Install Git page.</p> <p>To confirm the you have <code>git</code> installed correctly you can open a terminal window and type <code>git version</code>. You should receive a response like the one shown below.</p> <pre><code>git version\ngit version 2.39.5 (Apple Git-154)\n</code></pre>"},{"location":"labs/pre_work/#uv","title":"Uv","text":"<p><code>uv</code> is an extremely fast Python package and project manager, written in Rust.</p> <p>For detailed instructions on how to install <code>uv</code> on your laptop please refer to the Installing uv page where instructions for Mac, Windows and Linux machines can be found.</p> <p>To confirm the you have <code>uv</code> installed correctly you can open a terminal window and type <code>uv --version</code>. You should receive a response like the one shown below.</p> <pre><code>uv --version\nuv 0.6.12 (e4e03833f 2025-04-02)\n</code></pre>"},{"location":"labs/pre_work/#clone-the-granite-workshop-repository","title":"Clone the Granite Workshop Repository","text":"<p>Clone the workshop repository and cd into the repository directory.</p> <pre><code>git clone https://github.com/davidcolton/university_outreach_workshop\ncd university_outreach_workshop\n</code></pre>"},{"location":"labs/pre_work/#sync-the-python-virtual-environment","title":"Sync the Python Virtual Environment","text":"<p>The workshop repository uses a <code>pyproject.toml</code> file to define the version of Python to use and the required libraries to load. To sync your repository and setup Python and download the library dependancies run <code>uv sync</code> in a terminal. After syncing you have to activate your virtual environment.</p> <p>Note:</p> <p>If running on Windows it is suggested that you use the Windows Powershell running as administrator or, if you have it installed, the Windows Subsystem for Linux.</p> <pre><code>uv sync\n\n# Mac &amp; Linux\nsource .venv/bin/activate\n\n# Windows Powershell\n.venv\\Scripts\\activate\n</code></pre>"},{"location":"labs/pre_work/#running-ollama-locally","title":"Running Ollama Locally","text":"<p>If you want to run the AI models locally on your computer, you can use Ollama.</p> <p>Tested system</p> <p>This was tested on a Macbook with an M1 processor and 32GB RAM. It maybe possible to serve models with a CPU and less memory.</p> <p>Running Ollama locally on your computer requires the following steps:</p> <ol> <li> <p>Download and install Ollama, if you haven't already.</p> <p>On macOS, you can use Homebrew to install it with:</p> <pre><code>brew install ollama\n</code></pre> </li> <li> <p>Start the Ollama server. You will leave this running during the workshop.</p> <pre><code>ollama serve\n</code></pre> </li> <li> <p>In another terminal window, pull down the Granite models you will want to use in the workshop. Larger models take more memory to run but can give better results.</p> <pre><code>ollama pull granite4:micro\n</code></pre> </li> </ol>"},{"location":"labs/pre_work/#running-the-notebooks-remotely-colab","title":"Running the Notebooks Remotely (Colab)","text":"<p>If you are having difficulties getting your environment setup, or the workshop examples are not running successfully you can always run the Get Started with CoLab examples from the Mellea repository. Some of the content and examples may be different but you can still see the concepts shown here at work.</p> <p>Notebook execution speed tip</p> <p>The default execution runtime in Colab uses a CPU. Consider using a different Colab runtime to increase execution speed, especially in situations where you may have other constraints such as a slow network connection. From the navigation bar, select <code>Runtime -&gt; Change runtime type</code>, then select either GPU- or TPU-based hardware acceleration.</p>"},{"location":"labs/pre_work/#colab-prerequisites","title":"Colab Prerequisites","text":"<ul> <li>Google Colab requires a Google account that you're logged into</li> </ul>"}]}