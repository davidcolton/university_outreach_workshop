{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IBM University Outreach Workshop","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to our workshop! In this workshop we'll be using the open-sourced IBM Granite AI foundation models and Mellea for a number of use cases that demonstrates the value of generative AI.</p> <p>By the end of this workshop, you will be able to:</p> <ul> <li>Run a simple \"Hello Mellea\" program</li> <li>Use Mellea to Instruct, Validate, Repair</li> <li>Have some fun with <code>@generative</code> functions</li> <li>Use Mellea and Docling</li> <li>...</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Introduction</li> <li>About this workshop</li> <li>Agenda</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Link to Content Description Lab 0: Pre-work Pre-work for the workshop"},{"location":"#technology-used","title":"Technology Used","text":"<p>The technology used in the workshop is as follows:</p> <ul> <li>IBM Granite AI foundation models](https://www.ibm.com/granite)</li> <li>Mellea</li> <li>Docling</li> <li>Jupyter notebooks</li> <li>LangChain</li> <li>Ollama</li> </ul>"},{"location":"lab-1/readme/","title":"Document Summarization with Granite","text":"<p>Text summarization condenses one or more texts into shorter summaries for enhanced information extraction.</p> <p>The goal of this lab is to show how the IBM Granite models can be used in order to apply long document summarization techniques to a work of literature.</p>"},{"location":"lab-1/readme/#prerequisites","title":"Prerequisites","text":"<p>This lab is a Jupyter notebook. Please follow the instructions in pre-work to run the lab.</p>"},{"location":"lab-1/readme/#loading-the-lab","title":"Loading the Lab","text":"<p>Using colab to run the remotely {:target=\"_blank\"}</p> <p>To run the notebook from command line in Jupyter using the active virtual environment from the pre-work, run:</p> <pre><code>jupyter-lab\n</code></pre> <p>When Jupyter Lab opens the path to the <code>notebooks/Summarize.ipynb</code> notebook file is relative to the <code>WiDSIreland</code> folder from the git clone in the pre-work. The folder navigation pane on the left-hand side can be used to navigate to the file. Once the notebook has been found it can be double clicked and it will open to the pane on the right. </p>"},{"location":"lab-1/readme/#running-and-lab-with-explanations","title":"Running and Lab (with explanations)","text":"<p>This notebook demonstrates an application of long document summarisation techniques to a work of literature using Granite.</p> <p>The notebook contains both <code>code</code> cells and <code>markdown</code> text cells. The text cells gives a brief overview of the code and these cells are not executable. The code cells can be executed by placing the cursor in the cell and either hitting the Run this cell button at the top of the page or by pressing the <code>Shift</code> + <code>Enter</code> keys together. The main <code>code</code> cells are described in detail below.</p>"},{"location":"lab-1/readme/#selecting-your-model","title":"Selecting your model","text":"<p>Select a Granite model to use. Here we use a Langchain client to connect to the model. If there is a locally accessible Ollama server, we use an Ollama client to access the model. Otherwise, we use a Replicate client to access the model.</p> <p>When using Replicate, if the <code>REPLICATE_API_TOKEN</code> environment variable is not set, or a <code>REPLICATE_API_TOKEN</code> Colab secret is not set, then the notebook will ask for your Replicate API token in a dialog box.</p> <pre><code>try:  # Look for a locally accessible Ollama server for the model\n    response = requests.get(os.getenv(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\"))\n    model = OllamaLLM(\n        model=\"granite3.2:2b\",\n        num_ctx=65536,  # 64K context window\n    )\n    model = model.bind(raw=True)  # Client side controls prompt\nexcept Exception:  # Use Replicate for the model\n    model = Replicate(\n        model=\"ibm-granite/granite-3.2-8b-instruct\",\n        replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n        model_kwargs={\n            \"max_tokens\": 2000,  # Set the maximum number of tokens to generate as output.\n            \"min_tokens\": 200,  # Set the minimum number of tokens to generate as output.\n            \"temperature\": 0.75,\n            \"presence_penalty\": 0,\n            \"frequency_penalty\": 0,\n        },\n    )\n</code></pre> <p>In this first piece of code we try to determine if there is a local Ollama server running on <code>http://127.0.0.1:11434</code>. If the Ollama server is found then an <code>OllamaLLM</code> model instance is created for use later. If the Ollama server is not found the code then reverts to using the Granite 3.2-8b model served from Replicate.</p>"},{"location":"lab-1/readme/#chunk-document","title":"Chunk Document","text":"<pre><code>def chunk_document(\n    source: str,\n    *,\n    dropwhile: Callable[[BaseChunk], bool] = lambda c: False,\n    takewhile: Callable[[BaseChunk], bool] = lambda c: True,\n) -&gt; Iterator[BaseChunk]:\n    \"\"\"Read the document and perform a hierarchical chunking\"\"\"\n    converter = DocumentConverter()\n    chunks = HierarchicalChunker().chunk(converter.convert(source=source).document)\n    return itertools.takewhile(takewhile, itertools.dropwhile(dropwhile, chunks))\n</code></pre> <p>This Python function, <code>chunk_document</code>, is designed to perform hierarchical chunking on a given text document. Here's a breakdown of its components:</p> <ol> <li> <p>Function Signature: The function takes one required argument and two optional arguments. The required arguments are:</p> </li> <li> <p><code>source</code>: A string representing the text document to be chunked.</p> </li> <li><code>*</code>: Is a marker that all later arguments must be passed by keyword.</li> <li> <p>The optional arguments are:</p> <ul> <li><code>dropwhile</code> : A callable (function) that takes a <code>BaseChunk</code> object and returns a boolean. This function is used to determine when to stop dropping elements from the beginning of the chunks. The default is a lambda function that always returns <code>False</code>, meaning it will never drop any elements.</li> <li><code>takewhile</code>: A callable (function) that takes a <code>BaseChunk</code> object and returns a boolean. This function is used to determine when to stop taking elements from the beginning of the chunks. The default is a lambda function that always returns <code>True</code>, meaning it will take all elements.</li> </ul> </li> <li> <p>Document Conversion: The function first converts the input <code>source</code> string into a document object using a <code>DocumentConverter</code> instance (<code>converter</code> created on the first line of the function).</p> </li> <li> <p>Chunking: It then uses a <code>HierarchicalChunker</code> instance to perform hierarchical chunking on the document. The result is a list of <code>BaseChunk</code> objects.</p> </li> <li> <p>Itertools Dropwhile and Takewhile: Finally, the function uses <code>itertools.dropwhile</code> and <code>itertools.takewhile</code>to iterate over the chunks. The <code>dropwhile</code> function will drop elements from the beginning of the chunks as long as the <code>dropwhile</code> callable returns <code>True</code>. The <code>takewhile</code> </p> </li> </ol> <p>function will take elements from the remaining chunks as long as the <code>takewhile</code> callable returns <code>True</code>.</p> <ol> <li>Return Value: The function returns an iterator of <code>BaseChunk</code> objects, which represent the hierarchically chunked document.</li> </ol> <p>In summary, this function allows you to define custom conditions for dropping and taking chunks of a document, providing flexibility in how the document is segmented. The default behaviour is to not drop any chunks and to take all chunks.</p>"},{"location":"lab-1/readme/#merge-chunks","title":"Merge Chunks","text":"<pre><code>def merge_chunks(\n    chunks: Iterator[BaseChunk],\n    *,\n    headings: Callable[[BaseChunk], list[str]] = lambda c: c.meta.headings,\n) -&gt; Iterator[dict[str, str]]:\n    \"\"\"Merge chunks having the same headings\"\"\"\n    prior_headings: list[str] | None = None\n    document: dict[str, str] = {}\n    for chunk in chunks:\n        text = chunk.text.replace(\"\\r\\n\", \"\\n\")\n        current_headings = headings(chunk)\n        if prior_headings != current_headings:\n            if document:\n                yield document\n            prior_headings = current_headings\n            document = {\"title\": \" - \".join(current_headings), \"text\": text}\n        else:\n            document[\"text\"] += f\"\\n\\n{text}\"\n    if document:\n        yield document\n</code></pre> <ol> <li>Function Signature: The function is designed to merge chunks of text that share the same headings. It takes one required argument and one optional arguments. The required arguments are:</li> <li><code>chunks</code>: An an iterator of <code>BaseChunk</code> objects</li> <li><code>*</code>: Is a marker that all later arguments must be passed by keyword.</li> <li><code>headings</code>:An optional callable function that extracts headings from a chunk. The default function simply returns the <code>headings</code> attribute of the chunk's metadata.</li> <li>For each Chunk: The function iterates over each chunk in the input iterator. For each chunk, it extracts the text and headings. If the current chunk's headings differ from the previous chunk's headings, it yields the accumulated document (a dictionary with 'title' and 'text' keys) and starts a new document with the current chunk's headings and text. If the headings are the same, it appends the current chunk's text to the existing document.</li> <li>Finally: Finally, after the loop, if there's any remaining document (i.e., the last chunk in the iterator didn't end a section), it yields that document.</li> </ol> <p>The function returns an iterator of dictionaries, where each dictionary represents a merged chunk of text with its corresponding headings. The 'title' key in the dictionary is a concatenation of the headings with \" - \" as a separator, and the 'text' key contains the merged text of the chunks that share the same headings.</p>"},{"location":"lab-1/readme/#chunk-dropwhile","title":"Chunk Dropwhile","text":"<pre><code>def chunk_dropwhile(chunk: BaseChunk) -&gt; bool:\n    \"\"\"Ignore front matter prior to the book start\"\"\"\n    return \"WALDEN\" not in chunk.meta.headings\n</code></pre> <p>This Python function is designed to process chunks of data, specifically in the context of a document or book. The function takes one argument, <code>chunk</code>, which is expected to be an instance of a class or type named <code>BaseChunk</code>. This class or type is presumably defined elsewhere in the codebase and is likely used to represent a segment or part of a larger document.</p> <p>The <code>chunk</code> object has a property called <code>meta</code>, which is assumed to be an object containing metadata about the chunk. This metadata includes a list of headings, stored in <code>meta.headings</code>.</p> <p>The function checks if the string \"WALDEN\" is not in the list of headings. If \"WALDEN\" is not found in the headings, the function returns <code>True</code>, indicating that the chunk should be retained or processed further. If \"WALDEN\" is found in the headings, the function returns <code>False</code>, indicating that the chunk should be ignored or dropped.</p> <p>In essence, this function is used to filter out chunks that represent front matter (like a table of contents, preface, or introduction) before the main content of the book, which is assumed to start with the heading \"WALDEN\". This is a common pattern in text processing, where you want to skip over certain sections of a document.</p>"},{"location":"lab-1/readme/#chunk-takewhile","title":"Chunk Takewhile","text":"<pre><code>def chunk_takewhile(chunk: BaseChunk) -&gt; bool:\n    \"\"\"Ignore remaining chunks once we see this heading\"\"\"\n    return \"ON THE DUTY OF CIVIL DISOBEDIENCE\" not in chunk.meta.headings\n</code></pre> <p>This Python function, named <code>chunk_takewhile</code>, is designed to be used in a context where data is being processed in chunks. The function takes one argument, <code>chunk</code>, which is expected to be an instance of a class or subclass named <code>BaseChunk</code>.</p> <p>The purpose of this function is to determine whether to continue processing subsequent chunks or to stop processing based on the content of the current chunk. It does this by checking if a specific string, <code>\"ON THE DUTY OF CIVIL DISOBEDIENCE\"</code>, is present in the <code>headings</code> attribute of the <code>meta</code> attribute of the <code>chunk</code>  object.</p> <p>If the string is not found in the <code>headings</code>, the function returns <code>True</code>, indicating that the processing should continue with the next chunk. If the string is found, the function returns <code>False</code>, indicating that the processing should stop after the current chunk.</p> <p>In essence, this function acts as a filter or a condition for chunk processing, allowing you to control when to stop processing based on the content of the chunks. This can be particularly useful when dealing with large datasets or files that can be divided into smaller, more manageable chunks.</p>"},{"location":"lab-1/readme/#chunk-headings","title":"Chunk Headings","text":"<pre><code>def chunk_headings(chunk: BaseChunk) -&gt; list[str]:\n    \"\"\"Use the h1 and h2 (chapter) headings\"\"\"\n    return chunk.meta.headings[:2]\n</code></pre> <p>This Python function, named <code>chunk_headings</code>, is designed to extract the first two headings (h1 and h2) from a given chunk of content. The function takes one parameter, <code>chunk</code>, which is expected to be an instance of a class named <code>BaseChunk</code>.</p> <p>The <code>BaseChunk</code> class, which was imported from <code>docling_core.transforms.chunker.base</code> has a <code>meta</code> attribute, which itself is expected to be an object containing metadata about the chunk. This metadata object has a <code>headings</code> attribute, which is a list of strings representing the headings found in the chunk.</p> <p>The function returns a list containing the first two headings from this list. If there are fewer than two headings in the chunk, it will return all available headings.</p> <p>Here's a breakdown of the function:</p> <ul> <li>This function named <code>chunk_headings</code> takes one parameter,<code>chunk</code>, which is annotated to be an instance of <code>BaseChunk</code></li> <li>The function is expected to return a list of strings (<code>list[str]</code>).</li> <li><code>return chunk.meta.headings[:2]</code>. This line returns a slice of the <code>headings</code> list from the <code>meta</code> attribute of the <code>chunk</code> object. The slice <code>[:2]</code> indicates that only the first two elements of the list should be returned.</li> </ul> <p>In summary, this function is used to extract the first two headings (h1 and h2) from a chunk of content, which is used for creating a table of contents or for any other purpose that requires identifying the main sections of a document.</p>"},{"location":"lab-1/readme/#creating-the-document-objects-to-summarise","title":"Creating the Document Objects to Summarise","text":"<pre><code>documents: list[dict[str, str]] = list(\n    merge_chunks(\n        chunk_document(\n            \"https://www.gutenberg.org/cache/epub/205/pg205-images.html\",\n            dropwhile=chunk_dropwhile,\n            takewhile=chunk_takewhile,\n        ),\n        headings=chunk_headings,\n    )\n)\n</code></pre> <p>This Python code is fetching and processing a webpage from the Project Gutenberg website. Here's a breakdown:</p> <ol> <li><code>chunk_document</code>    This function is described in details earlier. It takes a URL and two other functions,<code>chunk_dropwhile</code> and <code>chunk_takewhile</code>, as arguments. It's responsible for fetching the webpage content and dividing it into smaller chunks based on the conditions defined by <code>chunk_dropwhile</code> and <code>chunk_takewhile</code>.</li> </ol> <p>might keep adding text to a chunk until a certain condition is no longer met. <code>chunk_headings</code> is another function that takes a chunk of text and returns a new chunk containing only the headings (HTML <code>&lt;h1&gt;</code> and <code>&lt;h2&gt;</code> tags) from the original chunk.</p> <ol> <li><code>merge_chunks</code>    Takes the chunks generated by <code>chunk_document</code> and combines them into a single list of dictionaries. Each dictionary represents a document with keys and values corresponding to the document's content and metadata, respectively.</li> </ol> <p>In summary, this code is fetching a webpage, dividing its content into chunks based on the defined conditions, extracting headings from each chunk, merging all chunks back into a single list, and storing the result as a list of dictionaries. Each dictionary in the list represents a document with its content and metadata.</p>"},{"location":"lab-1/readme/#summarise-chunks","title":"Summarise Chunks","text":"<p>Here we define a method to generate a response using a list of documents and a user prompt about those documents. We create the prompt according to the Granite Prompting Guide and provide the documents using the <code>documents</code> parameter.</p> <pre><code>def generate(user_prompt: str, documents: list[dict[str, str]]):\n    \"\"\"Use the chat template to format the prompt\"\"\"\n    prompt = tokenizer.apply_chat_template(\n        conversation=[\n            {\n                \"role\": \"user\",\n                \"content\": user_prompt,\n            }\n        ],\n        documents=documents,  # This uses the documents support in the Granite chat template\n        add_generation_prompt=True,\n        tokenize=False,\n    )\n\n    print(f\"Input size: {len(tokenizer.tokenize(prompt))} tokens\")\n    output = model.invoke(prompt)\n    print(f\"Output size: {len(tokenizer.tokenize(output))} tokens\")\n\n    return output\n</code></pre> <p>This Python function, named <code>generate</code> , is designed to interact with a language model, such as a chatbot or in our case our Granite Model, using a provided user prompt and a list of documents. Here's a breakdown of what the function does:</p> <ol> <li>The function takes two arguments:</li> <li><code>user_prompt</code>: a string representing the user's input or question.</li> <li><code>documents</code>: a list of dictionaries, where each dictionary contains key-value pairs representing the documents to be used as context for the language model.</li> <li>The function uses a tokenizer to apply a chat template to the user prompt and the provided documents. The chat template is a predefined format that structures the input for the language model. The <code>apply_chat_template</code> method formats the input as a conversation with a single \"user\" role and the user's prompt as the content. The <code>documents</code> argument is passed to the template to provide additional context for the model.</li> <li>The <code>add_generation_prompt</code> parameter is set to <code>True</code> , which means that a special generation prompt will be added to the input. This prompt guides the model to generate a response rather than just selecting an answer from a predefined set.</li> <li>The <code>tokenize</code> parameter is set to <code>False</code>, which means that the input and output will not be tokenized (i.e., broken down into individual words or subwords) before being passed to the model.</li> <li>The function then prints the size of the input in terms of the number of tokens (words or subwords) after tokenization.</li> <li>The <code>model.invoke(prompt)</code> line calls the language model with the formatted prompt and retrieves the generated output.</li> <li>The function prints the size of the output in terms of the number of tokens.</li> <li>Finally, the function returns the generated output from the language model.</li> </ol> <p>In summary, this function formats a user prompt and contextual documents using a chat template, invokes a language model with the formatted input, and returns the generated output. The token sizes are printed for debugging purposes.</p> <pre><code>user_prompt = \"\"\"\\\nUsing only the the book chapter document, compose a summary of the book chapter.\nYour response should only include the summary. Do not provide any further explanation.\"\"\"\n\nsummaries: list[dict[str, str]] = []\n\nfor document in documents:\n    print(\n        f\"============================= {document['title']} =============================\"\n    )\n    output = generate(user_prompt, [document])\n    summaries.append({\"title\": document[\"title\"], \"text\": output})\n\nprint(\"Summary count: \" + str(len(summaries)))\n</code></pre> <p>We then define the prompt we wist to use and invoke this <code>generate</code> function for each chapter creating a separate summary for each and populating a list of dictionaries with the chapter title and the summary from the model.</p>"},{"location":"lab-1/readme/#final-summary","title":"Final Summary","text":"<pre><code>user_prompt = \"\"\"\\\nUsing only the book chapter summary documents, compose a single, unified summary of the book.\nYour response should only include the unified summary. Do not provide any further explanation.\"\"\"\n\noutput = generate(user_prompt, summaries)\nprint(output)\n</code></pre> <p>To conclude we then call the <code>generate</code> function again but this time, instead of passing the full text of each chapter, we pass the list of dictionaries containing the chapter summaries and ask the model to provide an overall summary of the book by summarising the summaries of each chapter. So we have now summarized a document larger than the AI model's context window length by breaking the document down into smaller pieces to summarize and then summarizing those summaries.</p>"},{"location":"lab-1/readme/#credits","title":"Credits","text":"<p>This notebook is a modified version of the IBM Granite Community Document Summarization notebook. Refer to the IBM Granite Community for the official notebooks.</p>"},{"location":"lab-2/readme/","title":"Retrieval Augmented Generation (RAG) with Langchain","text":"<p>Retrieval Augumented Generation (RAG) is an architectural pattern that can be used to augment the performance of language models by recalling factual information from a knowledge base, and adding that information to the model query.</p> <p>The goal of this lab is to show how you can use RAG with an IBM Granite model to augment the model query answer using a publicly available document. The most common approach in RAG is to create dense vector representations of the knowledge base in order to retrieve text chunks that are semantically similar to a given user query.</p> <p>RAG use cases include: - Customer service: Answering questions about a product or service using facts from the product documentation. - Domain knowledge: Exploring a specialized domain (e.g., finance) using facts from papers or articles in the knowledge base. - News chat: Chatting about current events by calling up relevant recent news articles.</p> <p>In its simplest form, RAG requires 3 steps:</p> <ul> <li>Initial setup:</li> <li>Index knowledge-base passages for efficient retrieval. In this recipe, we take embeddings of the passages and store them in a vector database.</li> <li>Upon each user query:</li> <li>Retrieve relevant passages from the database. In this recipe, we use an embedding of the query to retrieve semantically similar passages.</li> <li>Generate a response by feeding retrieved passage into a large language model, along with the user query.</li> </ul>"},{"location":"lab-2/readme/#prerequisites","title":"Prerequisites","text":"<p>This lab is a Jupyter notebook. Please follow the instructions in pre-work to run the lab.</p>"},{"location":"lab-2/readme/#loading-the-lab","title":"Loading the Lab","text":"<p>Using colab to run the remotely {:target=\"_blank\"}</p> <p>To run the notebook from your command line in Jupyter using the active virtual environment from the pre-work, run:</p> <pre><code>jupyter-lab\n</code></pre> <p>When Jupyter Lab opens the path to the <code>notebooks/RAG_with_Langchain.ipynb</code> notebook file is relative to the <code>WiDSIrelandLab2025</code> folder from the git clone in the pre-work. The folder navigation pane on the left-hand side can be used to navigate to the file. Once the notebook has been found it can be double clicked and it will open to the pane on the right. </p>"},{"location":"lab-2/readme/#running-and-lab-with-explanations","title":"Running and Lab (with explanations)","text":"<p>This notebook demonstrates an application of long document summarisation techniques to a work of literature using Granite.</p> <p>The notebook contains both <code>code</code> cells and <code>markdown</code> text cells. The text cells each give a brief overview of the code in the following code cell(s). These cells are not executable. You can execute the code cells by placing your cursor in the cell and then either hitting the Run this cell button at the top of the page or by pressing the <code>Shift</code> + <code>Enter</code> keys together. The main <code>code</code> cells are described in detail below.</p>"},{"location":"lab-2/readme/#choosing-the-embeddings-model","title":"Choosing the Embeddings Model","text":"<pre><code>from langchain_huggingface import HuggingFaceEmbeddings\nfrom transformers import AutoTokenizer\n\nembeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\nembeddings_model = HuggingFaceEmbeddings(\n    model_name=embeddings_model_path,\n)\nembeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)\n</code></pre> <p>Here we are using the Hugging Face Transformers library to load a pre-trained model for generating embeddings (vector representations of text). Here's a breakdown of what each line does:</p> <ol> <li> <p><code>from langchain_huggingface import HuggingFaceEmbeddings</code>: This line imports the <code>HuggingFaceEmbeddings</code> class from the <code>langchain_huggingface</code> module. This class is used to load pre-trained models for generating embeddings.</p> </li> <li> <p><code>from transformers import AutoTokenizer</code>: This line imports the <code>AutoTokenizer</code> class from the <code>transformers</code> library. This class is used to tokenize text into smaller pieces (words, subwords, etc.) that can be processed by the model.</p> </li> <li> <p><code>embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"</code> : This line sets a variable <code>embeddings_model_path</code> to the path of the pre-trained model. In this case, it's a model called \"granite-embedding-30m-english\" developed by IBM's Granite project.</p> </li> <li> <p><code>embeddings_model = HuggingFaceEmbeddings(model_name=embeddings_model_path)</code>: This line creates an instance of the <code>HuggingFaceEmbeddings</code> class, loading the pre-trained model specified by <code>embeddings_model_path</code>.</p> </li> <li> <p><code>embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)</code>: This line creates an instance of the <code>AutoTokenizer</code> class, loading the tokenizer that was trained alongside the specified model. This tokenizer will be used to convert text into a format that the model can process.</p> </li> </ol> <p>In summary, we are setting up a system for generating embeddings from text using a pre-trained model and its associated tokenizer. The embeddings can then be used for various natural language processing tasks, such as text classification, clustering, or similarity comparison.</p> <p>To use a model from a provider other than Huggingface, replace this code cell with one from this Embeddings Model recipe.</p>"},{"location":"lab-2/readme/#choose-your-vector-database","title":"Choose your Vector Database","text":"<p>Specify the database to use for storing and retrieving embedding vectors.</p> <p>To connect to a vector database other than Milvus substitute this code cell with one from this Vector Store recipe.</p> <pre><code>from langchain_milvus import Milvus\nimport tempfile\n\ndb_file = tempfile.NamedTemporaryFile(prefix=\"milvus_\", suffix=\".db\", delete=False).name\nprint(f\"The vector database will be saved to {db_file}\")\n\nvector_db = Milvus(\n    embedding_function=embeddings_model,\n    connection_args={\"uri\": db_file},\n    auto_id=True,\n    index_params={\"index_type\": \"AUTOINDEX\"},\n)\n</code></pre> <p>This Python script is setting up a vector database using Milvus, a vector database built for AI applications, and Hugging Face's Transformers library for embeddings. It uses the previously created Embeddings Model. Here's a breakdown of what the code does:</p> <ol> <li>It imports <code>tempfile</code> and <code>Milvus</code> from <code>langchain_milvus</code>.</li> <li>It creates a temporary file for the Milvus database using <code>tempfile.NamedTemporaryFile()</code>. This file will store the vector database.</li> <li>It initializes an instance of <code>Milvus</code>with the embedding function set to the previously created <code>embeddings_model</code>. The connection arguments specify the URI of the database file, which is the temporary file created in the previous step. The <code>auto_id</code> parameter is set to True, which means Milvus will automatically generate IDs for the vectors. The <code>index_params</code> parameter sets the index type to \"AUTOINDEX\", which allows Milvus to automatically choose the most suitable index for the data.</li> </ol> <p>In summary, this script sets up a vector database using Milvus and a pre-trained embedding model from Hugging Face. The database is stored in a temporary file, and it's ready to index and search vector representations of text data.</p>"},{"location":"lab-2/readme/#selecting-your-model","title":"Selecting your model","text":"<p>Select a Granite model to use. Here we use a Langchain client to connect to  the model. If there is a locally accessible Ollama server, we use an  Ollama client to access the model. Otherwise, we use a Replicate client to access the model.</p> <p>When using Replicate, if the <code>REPLICATE_API_TOKEN</code> environment variable is not set, or a <code>REPLICATE_API_TOKEN</code> Colab secret is not set, then the notebook will ask for your Replicate API token in a dialog box.</p> <p><pre><code>model_path = \"ibm-granite/granite-3.2-8b-instruct\"\ntry:  # Look for a locally accessible Ollama server for the model\n    response = requests.get(os.getenv(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\"))\n    model = OllamaLLM(\n        model=\"granite3.2:2b\",\n    )\n    model = model.bind(raw=True)  # Client side controls prompt\nexcept Exception:  # Use Replicate for the model\n    model = Replicate(\n        model=model_path,\n        replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n    )\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n</code></pre> 1. <code>model_path = \"ibm-granite/granite-3.2-8b-instruct\"</code>: This line assigns the string <code>\"ibm-granite/granite-3.2-8b-instruct\"</code> to the <code>model_path</code> variable. This is the name of the pre-trained model on the Hugging Face Model Hub that will be used for the language model. 2. <code>try:</code>: This line starts a try block, which is used to handle exceptions that may occur during the execution of the code within the block. 3. <code>response = requests.get(os.getenv(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\"))</code>: This line sends a GET request to the Ollama server using the <code>requests.get()</code> function. The server address is obtained from the <code>OLLAMA_HOST</code> environment variable. If the environment variable is not set, the default address <code>http://127.0.0.1:11434</code> is used. 4. <code>model = OllamaLLM(model=\"granite3.2:2b\")</code>: This line creates an instance of the <code>OllamaLLM</code> class from the <code>ollama</code> library, specifying the model name as <code>\"granite3.2:2b\"</code>. 5. <code>model = model.bind(raw=True)</code>: This line binds the <code>OllamaLLM</code> instance to the client-side, allowing client-side controls over the prompt. 6. <code>except:</code>: This line starts an except block, which is used to handle exceptions that occur within the try block. 7. <code>model = Replicate(model=model_path, replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"))</code>: This line creates an instance of the <code>Replicate</code> class from the <code>replicate</code> library, specifying the model path and the Replicate API token obtained from the <code>REPLICATE_API_TOKEN</code> environment variable. 8. <code>tokenizer = AutoTokenizer.from_pretrained(model_path)</code>: This line loads a pre-trained tokenizer for the specified model using the <code>AutoTokenizer.from_pretrained()</code> method from the <code>transformers</code> library.</p> <p>In summary, the code snippet attempts to connect to a locally accessible Ollama server for the specified model. If the connection is successful, it creates an <code>OllamaLLM</code> instance and binds it to the client-side. If the connection fails, it uses the Replicate service to load the model. In both cases, a tokenizer is loaded for the specified model using the <code>AutoTokenizer.from_pretrained()</code> method.</p>"},{"location":"lab-2/readme/#building-the-vector-database","title":"Building the Vector Database","text":"<p>In this example, we take the State of the Union speech text, split it into chunks, derive embedding vectors using the embedding model, and load it into the vector database for querying.</p>"},{"location":"lab-2/readme/#download-the-document","title":"Download the document","text":"<p>Here we use President Biden's State of the Union address from March 1, 2022.</p> <pre><code>import os\nimport wget\n\nfilename = \"state_of_the_union.txt\"\nurl = \"https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt\"\n\nif not os.path.isfile(filename):\n    wget.download(url, out=filename)\n</code></pre> <ol> <li><code>filename = \"state_of_the_union.txt\"</code>: This line assigns the string <code>\"state_of_the_union.txt\"</code> to the <code>filename</code> variable. This is the name of the file that will be downloaded and saved locally.</li> <li><code>url = \"https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt\"</code>: This line assigns the URL of the file to be downloaded to the <code>url</code> variable.</li> <li><code>if not os.path.isfile(filename)</code>: This line checks if the file specified by <code>filename</code> does not already exist in the current working directory. The <code>os.path.isfile()</code> function returns <code>True</code> if the file exists and <code>False</code> otherwise.</li> <li><code>wget.download(url, out=filename)</code>: If the file does not exist, this line uses the <code>wget.download()</code> function to download the file from the specified URL and save it with the name <code>filename</code>. The <code>out</code> parameter is used to specify the output file name.</li> </ol> <p>In summary, the code snippet checks if a file with the specified name already exists in the current working directory. If the file does not exist, it downloads the file from the provided URL using the <code>wget</code> library and saves it with the specified filename.</p>"},{"location":"lab-2/readme/#split-the-document-into-chunks","title":"Split the document into chunks","text":"<pre><code>from langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\nloader = TextLoader(filename)\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n    tokenizer=embeddings_tokenizer,\n    chunk_size=embeddings_tokenizer.max_len_single_sentence,\n    chunk_overlap=0,\n)\ntexts = text_splitter.split_documents(documents)\nfor doc_id, text in enumerate(texts):\n    text.metadata[\"doc_id\"] = doc_id\nprint(f\"{len(texts)} text document chunks created\")\n</code></pre> <p>This Python script is using the Langchain library to load a text file and split it into smaller chunks. Here's a breakdown of what each part does:</p> <ol> <li><code>from langchain.document_loaders import TextLoader</code>: This line imports the TextLoader class from the langchain.document_loaders module. TextLoader is used to load documents from a file.</li> <li><code>from langchain.text_splitter import CharacterTextSplitter</code> : This line imports the CharacterTextSplitter class from the <code>langchain.text_splitter</code> module. <code>CharacterTextSplitter</code> is used to split text into smaller chunks.</li> <li><code>loader = TextLoader(filename)</code> : This line creates an instance of <code>TextLoader</code>, which is used to load the text from the specified file <code>(filename)</code>.</li> <li><code>documents = loader.load()</code> : This line loads the text from the file and stores it in the <code>documents</code> variable as a list of strings.</li> <li><code>text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(...)</code> : This line creates an instance of <code>CharacterTextSplitter</code>. It takes a Hugging Face tokenizer <code>(embeddings_tokenizer)</code>, sets the chunk size to the maximum length of a single sentence that the tokenizer can handle, and sets the chunk overlap to 0 (meaning no overlap between chunks).</li> <li><code>texts = text_splitter.split_documents(documents)</code>: This line splits the documents into smaller chunks using the <code>CharacterTextSplitter</code> instance. The result is stored in the texts variable as a list of lists, where each inner list contains the chunks of a single document.</li> <li><code>for doc_id, text in enumerate(texts): text.metadata[\"doc_id\"] = doc_id</code>: This loop assigns a unique identifier (doc_id) to each chunk of text. The doc_id is the index of the chunk in the texts list.</li> <li><code>print(f\"{len(texts)} text document chunks created\")</code>: This line prints the total number of text chunks created.</li> </ol> <p>In summary, this script loads a text file, splits it into smaller chunks based on the maximum sentence length that a Hugging Face tokenizer can handle, assigns a unique identifier to each chunk, and then prints the total number of chunks created.</p>"},{"location":"lab-2/readme/#populate-the-vector-database","title":"Populate the vector database","text":"<p>NOTE: Population of the vector database may take over a minute depending on your embedding model and service.</p> <pre><code>ids = vector_db.add_documents(texts)\nprint(f\"{len(ids)} documents added to the vector database\")\n</code></pre> <p>Next we load the <code>texts</code> object created earlier, split it into sentence-sized chunks, and adds these chunks to our vector database, associating each chunk with a unique ID.</p> <ol> <li><code>ids = vector_db.add_documents(texts)</code>: This line adds the text chunks to a vector database (<code>vector_db</code>). The <code>add_documents</code> method returns a list of IDs for the added documents.</li> <li><code>print(f\"{len(ids)} documents added to the vector database\")</code>: This line prints the number of documents added to the vector database.</li> </ol>"},{"location":"lab-2/readme/#querying-the-vector-database","title":"Querying the Vector Database","text":""},{"location":"lab-2/readme/#conduct-a-similarity-search","title":"Conduct a similarity search","text":"<p>Search the database for similar documents by proximity of the embedded vector in vector space.</p> <p><pre><code>query = \"What did the president say about Ketanji Brown Jackson?\"\ndocs = vector_db.similarity_search(query)\nprint(f\"{len(docs)} documents returned\")\nfor doc in docs:\n    print(doc)\n    print(\"=\" * 80)  # Separator for clarity\n</code></pre> 1. <code>query = \"What did the president say about Ketanji Brown Jackson?\"</code>: This line assigns the string <code>\"What did the president say about Ketanji Brown Jackson?\"</code> to the <code>query</code> variable. This is the search query that will be used to find relevant documents in the vector database. 2. <code>docs = vector_db.similarity_search(query)</code>: This line calls the <code>similarity_search()</code> method of the <code>vector_db</code> object, passing the <code>query</code> as an argument. The method returns a list of documents that are most similar to the query based on the vector representations of the documents in the vector database. 3. <code>print(f\"{len(docs)} documents returned\")</code>: This line prints the number of documents returned by the <code>similarity_search()</code> method. The <code>len()</code> function is used to determine the length of the <code>docs</code> list. 4. <code>for doc in docs:</code>: This line starts a loop that iterates over each document in the <code>docs</code> list. 5. <code>print(doc)</code>: This line prints the content of the current document in the loop. 6. <code>print(\"=\" * 80)</code>: This line prints a separator line consisting of 80 equal signs (<code>=</code>) to improve the readability of the output by visually separating the content of each document.</p> <p>In summary, the code snippet defines a search query, uses the <code>similarity_search()</code> method of a vector database to find relevant documents, and prints the number of documents returned along with their content. The separator line improves the readability of the output by visually separating the content of each document.</p>"},{"location":"lab-2/readme/#answering-questions","title":"Answering Questions","text":""},{"location":"lab-2/readme/#automate-the-rag-pipeline","title":"Automate the RAG pipeline","text":"<p>Build a RAG chain with the model and the document retriever.</p> <p>First we create the prompts for Granite to perform the RAG query. We use the Granite chat template and supply the placeholder values that the LangChain RAG pipeline will replace.</p> <p><code>{context}</code> will hold the retrieved chunks, as shown in the previous search, and feeds this to the model as document context for answering our question.</p> <p>Next, we construct the RAG pipeline by using the Granite prompt templates previously created.</p> <p><pre><code>from langchain.prompts import PromptTemplate\nfrom langchain.chains.retrieval import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\n# Create a Granite prompt for question-answering with the retrieved context\nprompt = tokenizer.apply_chat_template(\n    conversation=[{\n        \"role\": \"user\",\n        \"content\": \"{input}\",\n    }],\n    documents=[{\n        \"title\": \"placeholder\",\n        \"text\": \"{context}\",\n    }],\n    add_generation_prompt=True,\n    tokenize=False,\n)\nprompt_template = PromptTemplate.from_template(template=prompt)\n\n# Create a Granite document prompt template to wrap each retrieved document\ndocument_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\nDocument {doc_id}\n{page_content}\"\"\")\ndocument_separator=\"\\n\\n\"\n\n# Assemble the retrieval-augmented generation chain\ncombine_docs_chain = create_stuff_documents_chain(\n    llm=model,\n    prompt=prompt_template,\n    document_prompt=document_prompt_template,\n    document_separator=document_separator,\n)\nrag_chain = create_retrieval_chain(\n    retriever=vector_db.as_retriever(),\n    combine_docs_chain=combine_docs_chain,\n)\n</code></pre> 1. <code>from langchain.prompts import PromptTemplate</code>: This line imports the <code>PromptTemplate</code> class from the <code>langchain.prompts</code> module. This class is used to create custom prompt templates for language models. 2. <code>from langchain.chains.retrieval import create_retrieval_chain</code>: This line imports the <code>create_retrieval_chain()</code> function from the <code>langchain.chains.retrieval</code> module. This function is used to create a retrieval-augmented generation (RAG) chain, which combines a retrieval component (e.g., a vector database) with a language model for generating context-aware responses. 3. <code>from langchain.chains.combine_documents import create_stuff_documents_chain</code>: This line imports the <code>create_stuff_documents_chain()</code> function from the <code>langchain.chains.combine_documents</code> module. This function is used to create a chain that combines multiple retrieved documents into a single input for the language model. 4. <code>prompt = tokenizer.apply_chat_template(...)</code>: This line creates a custom prompt template for a question-answering task using the <code>apply_chat_template()</code> method of the <code>tokenizer</code> object. The prompt template includes a user role with the input question and a document role with the retrieved context. The <code>add_generation_prompt</code> parameter is set to <code>True</code> to include a generation prompt for the language model. 5. <code>prompt_template = PromptTemplate.from_template(template=prompt)</code>: This line creates a <code>PromptTemplate</code> object from the custom prompt template. 6. <code>document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\ Document {doc_id} {page_content}\"\"\")</code>: This line creates a custom prompt template for wrapping each retrieved document. The template includes a document identifier (<code>{doc_id}</code>) and the document content (<code>{page_content}</code>). 7. <code>document_separator=\"\\n\\n\"</code>: This line assigns the string <code>\"\\n\\n\"</code> to the <code>document_separator</code> variable. This separator will be used to separate the content of each retrieved document in the combined input for the language model. 8. <code>combine_docs_chain = create_stuff_documents_chain(...)</code>: This line creates a chain that combines multiple retrieved documents into a single input for the language model using the <code>create_stuff_documents_chain()</code> function. The function takes the language model (<code>model</code>), the prompt template (<code>prompt_template</code>), the document prompt template (<code>document_prompt_template</code>), and the document separator (<code>document_separator</code>) as arguments. 9. <code>rag_chain = create_retrieval_chain(...)</code>: This line creates a retrieval-augmented generation (RAG) chain using the <code>create_retrieval_chain()</code> function. The function takes the retrieval component (i.e., the vector database wrapped with <code>as_retriever()</code>) and the combine documents chain (<code>combine_docs_chain</code>) as arguments.</p> <p>In summary, the code snippet imports necessary classes and functions from the <code>langchain</code> library to create a retrieval-augmented generation (RAG) chain. It defines a custom prompt template for a question-answering task, creates a document prompt template for wrapping retrieved documents, and assembles the RAG chain by combining the retrieval component and the combine documents chain.</p>"},{"location":"lab-2/readme/#generate-a-retrieval-augmented-response-to-a-question","title":"Generate a retrieval-augmented response to a question","text":"<p>Use the RAG chain to process a question. The document chunks relevant to that question are retrieved and used as context.</p> <p><pre><code>output = rag_chain.invoke({\"input\": query})\n\nprint(output['answer'])\n</code></pre> 1. <code>output = rag_chain.invoke({\"input\": query})</code>: This line invokes the RAG chain with the input query. The <code>invoke()</code> method takes a dictionary as an argument, where the key is <code>\"input\"</code> and the value is the <code>query</code> string. The method returns a dictionary containing the output of the RAG chain, which includes the generated answer. 2. <code>print(output['answer'])</code>: This line prints the generated answer from the RAG chain output. The <code>output</code> dictionary is accessed using the key <code>'answer'</code>, which corresponds to the generated answer in the RAG chain's response.</p> <p>In summary, the code snippet invokes the RAG chain with the input query and prints the generated answer from the RAG chain's output.</p>"},{"location":"lab-2/readme/#credits","title":"Credits","text":"<p>This notebook is a modified version of the IBM Granite Community Retrieval Augmented Generation (RAG) with Langchain notebook. Refer to the IBM Granite Community for the official notebooks.</p>"},{"location":"lab-3/readme/","title":"Energy Demand Forecasting with Granite Timeseries (TTM)","text":"<p>TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. With less than 1 Million parameters, TTM introduces the notion of the first-ever \"tiny\" pre-trained models for Time-Series Forecasting. TTM outperforms several popular benchmarks demanding billions of parameters in zero-shot and few-shot forecasting and can easily be fine-tuned for multi-variate forecasts.</p>"},{"location":"lab-3/readme/#install-the-tsfm-library","title":"Install the TSFM Library","text":"<p>The granite-tsfm library provides utilities for working with Time Series Foundation Models (TSFM). Here the pinned version is retrieved and installed.</p>"},{"location":"lab-3/readme/#install-the-tsfm-library_1","title":"Install the tsfm library","text":"<pre><code>import sys\nif 'google.colab' in sys.modules:\n    ! pip install --force-reinstall --no-warn-conflicts \"numpy&lt;2.1.0,&gt;=2.0.2\"\n</code></pre> <ol> <li>This code snippet uses the \"granite-tsfm[notebooks]==v0.2.22\" library.</li> <li>The code imports the sys module and </li> <li>Checks if the google.colab module is present in the sys.modules dictionary and if found, it installs the numpy library with a specific version range (&lt;2.1.0, &gt;=2.0.2) using the pip  package manager. The --force-reinstall flag forces a reinstallation of the package, and the --no-warn-conflicts flag suppresses warnings about conflicting dependencies.</li> </ol>"},{"location":"lab-3/readme/#import-packages","title":"Import Packages","text":"<p>From <code>tsfm_public</code>, we use the TinyTimeMixer model, forecasting pipeline, and plotting function.</p> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\nfrom tsfm_public import (\n    TimeSeriesForecastingPipeline,\n    TinyTimeMixerForPrediction,\n)\nfrom tsfm_public.toolkit.visualization import plot_predictions\n</code></pre> <ol> <li>The first three lines in the snippet of code imports the matplotlib.pyplot module as plt, the pandas library as pd and the torch library. These libraries are utilizied for the data visualizations, dataframes and working with tensors which provides-multidimensional arrays respectively.</li> <li>The fourth line imports the TimeSeriesForecastingPipeline and TinyTimeMixerForPrediction classes from the tsfm_public module. These classes are part of the tsfm_public library, which is a collection of time series forecasting models and utilities.</li> <li>The fifth line imports the plot_predictions function from the visualization submodule of the tsfm_public library. </li> </ol> <p>This function is used to visualize the forecasted time series data.This code snippet imports the necessary libraries and classes for working with time series data, performing forecasting, and visualizing the results. The imported libraries and classes include data manipulation (Pandas), data visualization (Matplotlib), deep learning (Torch), and time series forecasting (tsfm_public).</p>"},{"location":"lab-3/readme/#download-the-data","title":"Download the data","text":"<p>We'll work with a dataset of hourly electrical demand, generation by type, and weather in Spain. This dataset was originally available from Kaggle. To simplify access to the data, we will make use of the (energy consumption and weather) datasets on Hugging Face.</p> <p><pre><code>DATA_FILE_PATH = \"hf://datasets/vitaliy-sharandin/energy-consumption-hourly-spain/energy_dataset.csv\"\n</code></pre> The code snippet defines a variable named DATA_FILE_PATH and assigns it a string value. This is a Hugging Face dataset identifier, which follows the format hf://datasets/username/dataset_name/dataset_file.format</p> <p><code>hf://datasets/vitaliy-sharandin/energy-consumption-hourly-spain/energy_dataset.csv</code>: points to a specific CSV file containing energy consumption data for Spain, with hourly granularity. </p>"},{"location":"lab-3/readme/#specify-time-and-output-variables","title":"Specify time and output variables","text":"<p>We provide the names of the timestamp column and the target column to be predicted. The context length (in time steps) is set to match the pretrained model.</p> <pre><code>timestamp_column = \"time\"\ntarget_columns = [\"total load actual\"]\ncontext_length = 512\n</code></pre> <ol> <li><code>timestamp_column</code>: This variable is assigned the string value \"time\". It represents the name of the column in the dataset that contains the timestamp information for each data point. In this case, the timestamp column is named \"time\".</li> <li><code>target_columns</code>: This variable is assigned a list containing the string value \"total load actual\". It represents the name of the column(s) in the dataset that contain the target variable(s) for the forecasting task. In this case, the target column is named \"total load actual\", which likely represents the actual total energy consumption at each time point.</li> <li><code>context_length</code>: This variable is assigned the integer value 512. It represents the number of time steps that the model will consider as context when generating forecasts. In this case, the context length is set to 512, meaning that the model will use the previous 512 time steps to predict the next time step.</li> </ol> <p>In summary, the code snippet defines three variables: timestamp_column, target_columns, and context_length. These variables are used to specify the timestamp column, target column(s), and context length for a time series forecasting task using the TinyTimeMixer model.</p>"},{"location":"lab-3/readme/#read-in-the-data","title":"Read in the data","text":"<p>We parse the csv into a pandas dataframe, filling in any null values, and create a single window containing <code>context_length</code> time points. We ensure the timestamp column is a datetime.</p> <pre><code># Read in the data from the downloaded file.\ninput_df = pd.read_csv(\n    DATA_FILE_PATH,\n    parse_dates=[timestamp_column],  # Parse the timestamp values as dates.\n)\n\n# Fill NA/NaN values by propagating the last valid value.\ninput_df = input_df.ffill()\n\n# Only use the last `context_length` rows for prediction.\ninput_df = input_df.iloc[-context_length:,]\n\n# Show the last few rows of the dataset.\ninput_df.tail()\n</code></pre> <ol> <li><code>pd.read_csv()</code>: This function from the Pandas library is used to read a CSV file into a DataFrame. In this case, it reads the data from the file specified by <code>DATA_FILE_PATH</code>.</li> <li><code>parse_dates=[timestamp_column]</code>: This argument is passed to the <code>pd.read_csv()</code> function to specify that the column named <code>timestamp_column</code> should be parsed as dates. This ensures that the timestamp values are treated as datetime objects, allowing for more accurate time-based operations and visualizations.</li> <li><code>input_df.ffill()</code>: This method is called on the <code>input_df</code> DataFrame to fill any missing or NaN values by propagating the last valid value. This is a common technique for handling missing data in time series datasets, as it maintains the temporal order of the data.</li> <li><code>input_df.iloc[-context_length:, ]</code>: This method is used to select the last <code>context_length</code> rows from the <code>input_df</code> DataFrame. This ensures that only the most recent data is used for prediction, as the TinyTimeMixer model requires a fixed-length context window.</li> <li><code>input_df.tail()</code>: This method is called on the <code>input_df</code> DataFrame to display the last few rows of the dataset. This can be useful for quickly verifying that the data has been correctly loaded, preprocessed, and filtered.</li> </ol> <p>In summary, the code snippet reads a CSV file containing time series data, parses the timestamp column as dates, fills missing values using forward propagation, selects the last <code>context_length</code> rows for prediction, and displays the last few rows of the dataset. These steps prepare the data for use with the TinyTimeMixer model in a time series forecasting task.</p>"},{"location":"lab-3/readme/#plot-the-target-series","title":"Plot the target series","text":"<p>Here we inspect a preview of the target time series column.</p> <pre><code>fig, axs = plt.subplots(len(target_columns), 1, figsize=(10, 2 * len(target_columns)), squeeze=False)\nfor ax, target_column in zip(axs, target_columns):\n    ax[0].plot(input_df[timestamp_column], input_df[target_column])\n</code></pre> <ol> <li><code>plt.subplots()</code>: This function from the Matplotlib library is used to create a figure and a set of subplots. In this case, it creates a figure with a grid of subplots, where the number of rows is equal to the length of the <code>target_columns</code> list, and the number of columns is 1. The <code>figsize</code> argument is used to set the size of the figure, and the <code>squeeze</code> argument is set to <code>False</code> to ensure that the returned axes object is always a 2D array, even if there is only one subplot.</li> <li><code>for ax, target_column in zip(axs, target_columns)</code>: This loop iterates over the rows of the <code>axs</code> array (i.e., the subplots) and the <code>target_columns</code> list simultaneously using the <code>zip()</code> function. For each iteration, the loop assigns the current subplot (<code>ax</code>) and the corresponding target column (<code>target_column</code>) to the variables <code>ax</code> and <code>target_column</code>, respectively.</li> <li><code>ax[0].plot(input_df[timestamp_column], input_df[target_column])</code>: Inside the loop, this line plots the time series data for the current target column on the corresponding subplot. The <code>input_df[timestamp_column]</code> expression retrieves the timestamp values, and the <code>input_df[target_column]</code> expression retrieves the target column values. The <code>plot()</code> method is called on the current subplot (<code>ax[0]</code>) to create the line plot.</li> </ol> <p>In summary, the code snippet creates a figure with a grid of subplots, one for each target column, and plots the time series data for each target column on its corresponding subplot. This visualization helps to compare the trends and patterns in the target columns over time.</p>"},{"location":"lab-3/readme/#set-up-zero-shot-model","title":"Set up zero shot model","text":"<p>The TTM model is hosted on Hugging Face, and is retrieved by the wrapper, <code>TinyTimeMixerForPrediction</code>. We have one input channel in this example.</p> <p><pre><code># Instantiate the model.\nzeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\n    \"ibm-granite/granite-timeseries-ttm-r2\",  # Name of the model on Hugging Face\n    num_input_channels=len(target_columns),  # tsp.num_input_channels\n)\n</code></pre> 1. <code>TinyTimeMixerForPrediction.from_pretrained()</code>: This method is used to instantiate a pre-trained TinyTimeMixer model from the Hugging Face Model Hub. The method takes the following arguments:     * <code>\"ibm-granite/granite-timeseries-ttm-r2\"</code>: The name of the pre-trained model on the Hugging Face Model Hub. In this case, it is the TinyTimeMixer model pre-trained on time series data by IBM Granite.     * <code>num_input_channels</code>: The number of input channels in the time series data. This value is set to the length of the <code>target_columns</code> list, indicating that the model will process multiple target columns as separate input channels. 2. <code>zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(...)</code>: This line instantiates the pre-trained TinyTimeMixer model and assigns it to the <code>zeroshot_model</code> variable. The model is now ready to be used for forecasting tasks.</p> <p>In summary, the code snippet instantiates a pre-trained TinyTimeMixer model from the Hugging Face Model Hub, specifying the number of input channels based on the number of target columns in the dataset. The instantiated model is stored in the <code>zeroshot_model</code> variable and can be used for generating forecasts.</p>"},{"location":"lab-3/readme/#create-a-forecasting-pipeline","title":"Create a forecasting pipeline","text":"<p>Set up the forecasting pipeline with the model, setting <code>frequency</code> given our knowledge of the sample frequency. In this example we set <code>explode_forecasts</code> to <code>False</code>, which keeps each sequence of predictions in a list within the dataframe cells. We then make a forecast on the dataset.</p> <pre><code># Create a pipeline.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npipeline = TimeSeriesForecastingPipeline(\n    zeroshot_model,\n    timestamp_column=timestamp_column,\n    id_columns=[],\n    target_columns=target_columns,\n    explode_forecasts=False,\n    freq=\"h\",\n    device=device,  # Specify your local GPU or CPU.\n)\n\n# Make a forecast on the target column given the input data.\nzeroshot_forecast = pipeline(input_df)\nzeroshot_forecast.tail()\n</code></pre> <ol> <li><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"</code>: This line checks if a GPU is available for use and sets the <code>device</code> variable accordingly. If a GPU is available, the <code>device</code> variable is set to <code>\"cuda\"</code>; otherwise, it is set to <code>\"cpu\"</code>. This ensures that the forecasting pipeline uses the available hardware for faster computation.</li> <li><code>TimeSeriesForecastingPipeline</code>: This is a class from the <code>tsfm_public</code> library that represents a high-level interface for building and executing time series forecasting pipelines. The pipeline consists of several components, such as data preprocessing, feature engineering, model training, and forecast generation.</li> <li><code>pipeline = TimeSeriesForecastingPipeline(...)</code>: This line creates an instance of the <code>TimeSeriesForecastingPipeline</code> class, passing the required arguments to configure the pipeline. The arguments include:<ul> <li><code>zeroshot_model</code>: The TinyTimeMixer model used for forecasting.</li> <li><code>timestamp_column</code>: The name of the column containing the timestamp information.</li> <li><code>id_columns</code>: A list of columns that do not contain time series data and should not be used for forecasting. In this case, it is an empty list.</li> <li><code>target_columns</code>: A list of columns containing the target time series data for forecasting.</li> <li><code>explode_forecasts</code>: A boolean flag indicating whether to generate separate forecasts for each unique identifier in the <code>id_columns</code>. In this case, it is set to <code>False</code>.</li> <li><code>freq</code>: The frequency of the time series data, specified as a string (e.g., <code>\"h\"</code> for hourly data).</li> <li><code>device</code>: The device to use for computation, either <code>\"cuda\"</code> for a GPU or <code>\"cpu\"</code> for the CPU.</li> </ul> </li> <li><code>zeroshot_forecast = pipeline(input_df)</code>: This line uses the configured pipeline to generate forecasts for the target columns in the <code>input_df</code> DataFrame. The resulting forecasts are stored in the <code>zeroshot_forecast</code> variable.</li> <li><code>zeroshot_forecast.tail()</code>: This method is called on the <code>zeroshot_forecast</code> object to display the last few rows of the forecasted data. This can be useful for quickly verifying that the forecasts have been generated correctly.</li> </ol> <p>In summary, the code snippet creates a time series forecasting pipeline using the TinyTimeMixer model, configures the pipeline with the required parameters, generates forecasts for the target columns in the input data, and displays the last few rows of the forecasted data.</p>"},{"location":"lab-3/readme/#plot-predictions-along-with-the-historical-data","title":"Plot predictions along with the historical data.","text":"<p>The predicted series picks up where the historical data ends, and we can see that it predicts a continuation of the cyclical pattern and an upward trend.</p> <pre><code># Plot the historical data and predicted series.\nplot_predictions(\n    input_df=input_df,\n    predictions_df=zeroshot_forecast,\n    freq=\"h\",\n    timestamp_column=timestamp_column,\n    channel=target_column,\n    indices=[-1],\n    num_plots=1,\n)\n</code></pre> <ol> <li><code>plot_predictions()</code>: This function from the <code>tsfm_public.toolkit.visualization</code> module is used to visualize the historical data and predicted series. It takes several arguments to customize the plot:<ul> <li><code>input_df</code>: The input DataFrame containing the historical time series data.</li> <li><code>predictions_df</code>: The DataFrame containing the forecasted series.</li> <li><code>freq</code>: The frequency of the time series data, specified as a string (e.g., <code>\"h\"</code> for hourly data).</li> <li><code>timestamp_column</code>: The name of the column containing the timestamp information.</li> <li><code>channel</code>: The name of the target column for which the forecasts were generated.</li> <li><code>indices</code>: A list of indices specifying which forecasts to plot. In this case, it is set to <code>[-1]</code>, which means that only the most recent forecast is plotted.</li> <li><code>num_plots</code>: The number of subplots to create. In this case, it is set to <code>1</code>, indicating that a single plot is generated.</li> </ul> </li> <li><code>plot_predictions(...)</code>: This line calls the <code>plot_predictions()</code> function, passing the required arguments to customize the plot. The function generates a plot that displays the historical data and the most recent forecast for the specified target column.</li> </ol> <p>In summary, the code snippet uses the <code>plot_predictions()</code> function to visualize the historical data and the most recent forecast for the target column in a single plot. This visualization helps to compare the actual data with the predicted series and assess the model's performance.</p>"},{"location":"pre-work/readme/","title":"Pre-work","text":"<p>The labs in this workshop are Jupyter notebooks. Check out Running the Granite Notebooks section on how to setup the way you want to run the notebooks.</p> <ul> <li>Pre-work</li> <li>Running the Granite Notebooks Locally</li> <li>Local Prerequisites<ul> <li>Git</li> <li>Uv</li> </ul> </li> <li>Clone the Granite Workshop Repository<ul> <li>Sync the Python Virtual Environment</li> <li>Serving the Granite AI Models</li> <li>Replicate AI Cloud Platform</li> <li>Running Ollama Locally</li> </ul> </li> <li>Running the Granite Notebooks Remotely (Colab)<ul> <li>Colab Prerequisites</li> <li>Serving the Granite AI Models for Colab</li> <li>Replicate AI Cloud Platform for Colab</li> </ul> </li> </ul>"},{"location":"pre-work/readme/#running-the-granite-notebooks","title":"Running the Granite Notebooks","text":"<p>It is recommended if you want to run the lab notebooks locally on your computer that you have:</p> <ul> <li>A computer or laptop</li> <li>Knowledge of Git and Python</li> </ul> <p>Running the lab notebooks locally on your computer requires the following steps:</p>"},{"location":"pre-work/readme/#local-prerequisites","title":"Local Prerequisites","text":"<ul> <li>Git</li> <li>Uv</li> </ul>"},{"location":"pre-work/readme/#git","title":"Git","text":"<p>Git can be installed on most common operating systems like Windows,  Mac, and Linux. In fact, Git comes installed by default on most Mac and  Linux machines!</p> <p>For comprehensive instructions on how to install <code>git</code> on your laptop please refer to the Install Git page.</p> <p>To confirm the you have <code>git</code> installed correctly you can open a terminal window and type <code>git version</code>. You should receive a response like the one shown below.</p> <pre><code>git version\ngit version 2.39.5 (Apple Git-154)\n</code></pre>"},{"location":"pre-work/readme/#uv","title":"Uv","text":"<p><code>uv</code> is an extremely fast Python package and project manager, written in Rust.</p> <p>For detailed instructions on how to install <code>uv</code> on your laptop please refer to the Installing uv page where instructions for Mac, Windows and Linux machines can be found.</p> <p>To confirm the you have <code>uv</code> installed correctly you can open a terminal window and type <code>uv --version</code>. You should receive a response like the one shown below.</p> <pre><code>uv --version\nuv 0.6.12 (e4e03833f 2025-04-02)\n</code></pre>"},{"location":"pre-work/readme/#clone-the-granite-workshop-repository","title":"Clone the Granite Workshop Repository","text":"<p>Clone the workshop repository and cd into the repository directory.</p> <pre><code>git clone https://github.com/davidcolton/university_outreach_workshop\ncd university_outreach_workshop\n</code></pre>"},{"location":"pre-work/readme/#sync-the-python-virtual-environment","title":"Sync the Python Virtual Environment","text":"<p>The workshop repository uses a <code>pyproject.toml</code> file to define the version of Python to use and the required libraries to load. To sync your repository and setup Python and download the library dependancies run <code>uv sync</code> in a terminal. After syncing you have to activate your virtual environment.</p> <p>Note:</p> <p>If running on Windows it is suggested that you use the Windows Powershell running as administrator or, if you have it installed, the Windows Subsystem for Linux.</p> <pre><code>uv sync\n\n# Mac &amp; Linux\nsource .venv/bin/activate\n\n# Windows Powershell\n.venv\\Scripts\\activate\n</code></pre>"},{"location":"pre-work/readme/#running-ollama-locally","title":"Running Ollama Locally","text":"<p>If you want to run the AI models locally on your computer, you can use Ollama.</p> <p>Tested system</p> <p>This was tested on a Macbook with an M1 processor and 32GB RAM. It maybe possible to serve models with a CPU and less memory.</p> <p>Running Ollama locally on your computer requires the following steps:</p> <ol> <li> <p>Download and install Ollama, if you haven't already.</p> <p>On macOS, you can use Homebrew to install it with:</p> <pre><code>brew install ollama\n</code></pre> </li> <li> <p>Start the Ollama server. You will leave this running during the workshop.</p> <pre><code>ollama serve\n</code></pre> </li> <li> <p>In another terminal window, pull down the Granite models you will want to use in the workshop. Larger models take more memory to run but can give better results.</p> <pre><code>ollama pull granite4:micro\n</code></pre> </li> </ol>"},{"location":"pre-work/readme/#running-the-notebooks-remotely-colab","title":"Running the Notebooks Remotely (Colab)","text":"<p>If you are having difficulties getting your environment setup, or the workshop examples are not running successfully you can always run the Get Started with CoLab examples from the Mellea repository. Some of the content and examples may be different but you can still see the concepts shown here at work.</p> <p>Notebook execution speed tip</p> <p>\u200b    The default execution runtime in Colab uses a CPU. Consider using a different Colab runtime to increase execution speed, especially in situations where you may have other constraints such as a slow network connection. From the navigation bar, select <code>Runtime-&gt;Change runtime type</code>, then select either GPU- or TPU-based hardware acceleration.</p>"},{"location":"pre-work/readme/#colab-prerequisites","title":"Colab Prerequisites","text":"<ul> <li>Google Colab requires a Google account that you're logged into</li> </ul>"}]}